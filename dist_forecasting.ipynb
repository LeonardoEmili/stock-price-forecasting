{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e862fc71-84e1-4af7-badc-898efadd0bc5","showTitle":false,"title":""}},"source":["## Import required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d89c4672-97c1-49d3-9350-6008885da2ab","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"><span class=\"ansi-yellow-fg\">WARNING: You are using pip version 20.2.4; however, version 21.1.2 is available.\r\n","You should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","wandb: W&amp;B API key is configured (use `wandb login --relogin` to force relogin)\n","wandb: WARNING If you&#39;re specifying your api key in code, ensure this code is not shared publically.\n","wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"><span class=\"ansi-yellow-fg\">WARNING: You are using pip version 20.2.4; however, version 21.1.2 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: W&amp;B API key is configured (use `wandb login --relogin` to force relogin)\nwandb: WARNING If you&#39;re specifying your api key in code, ensure this code is not shared publically.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Download required packages\n","!pip -q install gdown missingno torch petastorm wandb\n","\n","from petastorm.pytorch import DataLoader\n","# Disable petastorm.DataLoader annoying logger (enable when debugging)\n","import logging.config\n","logging.config.dictConfig({'version': 1, 'disable_existing_loggers': True})\n","\n","%matplotlib inline\n","\n","import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.types import *\n","import pyspark.sql.functions as F\n","from pyspark import SparkContext, SparkConf\n","from pyspark.sql.window import Window\n","\n","from petastorm.spark import SparkDatasetConverter, make_spark_converter\n","spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, 'file:///dbfs/tmp/petastorm/cache')\n","\n","from pyspark.ml.regression import RandomForestRegressor, DecisionTreeRegressor\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.linalg import Vectors, VectorUDT\n","from pyspark.sql.functions import udf\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","import missingno as msno\n","import torch\n","import re\n","import torch.nn as nn\n","from typing import *\n","import datetime\n","import gdown\n","import dataclasses\n","from operator import itemgetter\n","from functools import partial\n","\n","import wandb\n","wandb.login(key='147339090e59f6a02bed0fa3f938a2b1ecbc567c')\n","\n","import tqdm as tq\n","def tqdm(*args, **kwargs):\n","  ''' Small trick to prevent tqdm printing newlines at each step. '''\n","  return tq.tqdm(*args, **kwargs, leave=True, position=0)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f76226d3-37ac-40d6-a417-a05ca05a7000","showTitle":false,"title":""}},"source":["## Data aquisition\n","We retrieve our datasets and download them to a temporary directory in the driver node."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7c1efdeb-4084-4115-9920-536feac86d20","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Downloading...\n","From: https://drive.google.com/uc?id=1ggmDp-AWFzbQReLG0pLpQE_3fO0C0RnM\n","To: /tmp/data.zip\n","\r0.00B [00:00, ?B/s]\r4.72MB [00:00, 31.5MB/s]\r8.91MB [00:00, 16.8MB/s]\r25.7MB [00:00, 28.9MB/s]\r42.5MB [00:01, 44.2MB/s]\r50.9MB [00:01, 47.0MB/s]\r59.2MB [00:01, 42.2MB/s]\r71.9MB [00:01, 44.6MB/s]\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Downloading...\nFrom: https://drive.google.com/uc?id=1ggmDp-AWFzbQReLG0pLpQE_3fO0C0RnM\nTo: /tmp/data.zip\n\r0.00B [00:00, ?B/s]\r4.72MB [00:00, 31.5MB/s]\r8.91MB [00:00, 16.8MB/s]\r25.7MB [00:00, 28.9MB/s]\r42.5MB [00:01, 44.2MB/s]\r50.9MB [00:01, 47.0MB/s]\r59.2MB [00:01, 42.2MB/s]\r71.9MB [00:01, 44.6MB/s]\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["!rm -rf /tmp/data /tmp/__MACOSX\n","gdown.download('https://drive.google.com/uc?id=1ggmDp-AWFzbQReLG0pLpQE_3fO0C0RnM', '/tmp/data.zip', quiet=False)\n","!unzip -q /tmp/data.zip -d /tmp/\n","!rm /tmp/data.zip"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"941ae011-778f-4659-9774-ff7df196b5b1","showTitle":false,"title":""}},"source":["Then we load the datasets to the DBFS."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"791fea85-d7b5-4432-b530-ddae393cc5d6","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[3]: True</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[3]: True</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["dbutils.fs.mv(\"file:/tmp/data\", \"dbfs:/data\", recurse=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c18efa6b-4546-4cab-9e4b-582119acee64","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/data/.DS_Store</td><td>.DS_Store</td><td>6148</td></tr><tr><td>dbfs:/data/key_stats_yahoo.csv</td><td>key_stats_yahoo.csv</td><td>2047081</td></tr><tr><td>dbfs:/data/prices/</td><td>prices/</td><td>0</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["dbfs:/data/.DS_Store",".DS_Store",6148],["dbfs:/data/key_stats_yahoo.csv","key_stats_yahoo.csv",2047081],["dbfs:/data/prices/","prices/",0]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":[],"xColumns":[],"yColumns":[]},"removedWidgets":[],"schema":[{"metadata":"{}","name":"path","type":"\"string\""},{"metadata":"{}","name":"name","type":"\"string\""},{"metadata":"{}","name":"size","type":"\"long\""}],"type":"table"}},"output_type":"display_data"}],"source":["%fs ls /data/"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f5e7b9cc-713e-4303-a194-15a532c0757b","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/data/.DS_Store</td><td>.DS_Store</td><td>6148</td></tr><tr><td>dbfs:/data/key_stats_yahoo.csv</td><td>key_stats_yahoo.csv</td><td>2047081</td></tr><tr><td>dbfs:/data/prices/</td><td>prices/</td><td>0</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["dbfs:/data/.DS_Store",".DS_Store",6148],["dbfs:/data/key_stats_yahoo.csv","key_stats_yahoo.csv",2047081],["dbfs:/data/prices/","prices/",0]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":[],"xColumns":[],"yColumns":[]},"removedWidgets":[],"schema":[{"metadata":"{}","name":"path","type":"\"string\""},{"metadata":"{}","name":"name","type":"\"string\""},{"metadata":"{}","name":"size","type":"\"long\""}],"type":"table"}},"output_type":"display_data"}],"source":["%fs ls /data/"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"898c47b5-01c8-4a5c-86c1-58bd6334f04a","showTitle":false,"title":""}},"source":["## Dataset loading\n","\n","We load the initial datasets into PySpark dataframes: one for fundamentals data and one for prices of every stock. We filter some selected stocks to make the execution feasible in short times on few nodes."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["key_stats_df = spark.read.load(\"dbfs:/data/key_stats_yahoo.csv\", \n","                           format=\"csv\",\n","                           sep=\",\",\n","                           inferSchema=\"true\",\n","                           header=\"true\"\n","                          )\n","\n","chosen_stocks = [\"aapl\", \"msft\", \"ge\", \"pfe\", \"t\"]\n","key_stats_df = key_stats_df.drop(key_stats_df.columns[0])\n","# Filtering chosen stocks\n","stock_dfs = []\n","for stock in chosen_stocks:\n","    stock_dfs.append(key_stats_df.filter(key_stats_df[\"Ticker\"] == stock))\n","\n","key_stats_df = stock_dfs[0]\n","for i in range(1, len(stock_dfs)):\n","    key_stats_df = key_stats_df.unionAll(stock_dfs[i])\n","\n","key_stats_df.schema['Date'].nullable = False\n","\n","# Use legacy format to parse dates\n","spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n","key_stats_df = key_stats_df.withColumn(\"Date\", F.to_date(key_stats_df[\"Date\"], 'MM/dd/yyyy HH:mm'))\n","\n","# Cast numerical columns to double\n","for column in key_stats_df.columns[2:]:\n","  key_stats_df = key_stats_df.withColumn(column, key_stats_df[column].cast(\"double\"))\n","\n","# Prices dataframes for every stock\n","stock_files =  list(map(lambda x: x + \".csv\", chosen_stocks))\n","prices_files = [f.path for f in dbutils.fs.ls('/data/prices/') if f.path.lower().split(\"/\")[-1] in stock_files]\n","dfs_names = [f.rsplit('/', 1)[1][:-len('.csv')] for f in prices_files]\n","prices_dfs = []\n","for f in tqdm(prices_files, desc='Reading stock price data', total=len(prices_files)):\n","  df = spark.read.load(\n","    f,\n","    format=\"csv\",\n","    sep=\",\",\n","    inferSchema=\"true\",\n","    header=\"true\"\n","  )\n","  df = df.withColumn(\"Date\", F.to_date(df[\"Date\"], 'dd-MM-yyyy'))\n","  df.schema['Date'].nullable = False\n","  prices_dfs.append(df)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"be1bb73d-1713-4401-9ca9-cef3f47a976f","showTitle":false,"title":""}},"source":["## Dataset analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"bd6040d0-f97c-4ef4-bd0d-3a1a8d4214a5","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Prices dataframe format:\n","root\n","-- Date: date (nullable = true)\n","-- Low: double (nullable = true)\n","-- Open: double (nullable = true)\n","-- Volume: integer (nullable = true)\n","-- High: double (nullable = true)\n","-- Close: double (nullable = true)\n","-- Adjusted Close: double (nullable = true)\n","\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Prices dataframe format:\nroot\n |-- Date: date (nullable = true)\n |-- Low: double (nullable = true)\n |-- Open: double (nullable = true)\n |-- Volume: integer (nullable = true)\n |-- High: double (nullable = true)\n |-- Close: double (nullable = true)\n |-- Adjusted Close: double (nullable = true)\n\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["print(\"Prices dataframe format:\")\n","prices_dfs[0].printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"dc56a262-df2a-4a39-ab77-76f55ee2948c","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Key stats dataframe format:\n","root\n","-- Date: date (nullable = true)\n","-- Ticker: string (nullable = true)\n","-- Price: double (nullable = true)\n","-- DE Ratio: double (nullable = true)\n","-- Trailing P/E: double (nullable = true)\n","-- Price/Sales: double (nullable = true)\n","-- Price/Book: double (nullable = true)\n","-- Profit Margin: double (nullable = true)\n","-- Operating Margin: double (nullable = true)\n","-- Return on Assets: double (nullable = true)\n","-- Return on Equity: double (nullable = true)\n","-- Revenue Per Share: double (nullable = true)\n","-- Market Cap: double (nullable = true)\n","-- Enterprise Value: double (nullable = true)\n","-- Forward P/E: double (nullable = true)\n","-- PEG Ratio: double (nullable = true)\n","-- Enterprise Value/Revenue: double (nullable = true)\n","-- Enterprise Value/EBITDA: double (nullable = true)\n","-- Revenue: double (nullable = true)\n","-- Gross Profit: double (nullable = true)\n","-- EBITDA: double (nullable = true)\n","-- Net Income Avl to Common : double (nullable = true)\n","-- Diluted EPS: double (nullable = true)\n","-- Earnings Growth: double (nullable = true)\n","-- Revenue Growth: double (nullable = true)\n","-- Total Cash: double (nullable = true)\n","-- Total Cash Per Share: double (nullable = true)\n","-- Total Debt: double (nullable = true)\n","-- Current Ratio: double (nullable = true)\n","-- Book Value Per Share: double (nullable = true)\n","-- Cash Flow: double (nullable = true)\n","-- Beta: double (nullable = true)\n","\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Key stats dataframe format:\nroot\n |-- Date: date (nullable = true)\n |-- Ticker: string (nullable = true)\n |-- Price: double (nullable = true)\n |-- DE Ratio: double (nullable = true)\n |-- Trailing P/E: double (nullable = true)\n |-- Price/Sales: double (nullable = true)\n |-- Price/Book: double (nullable = true)\n |-- Profit Margin: double (nullable = true)\n |-- Operating Margin: double (nullable = true)\n |-- Return on Assets: double (nullable = true)\n |-- Return on Equity: double (nullable = true)\n |-- Revenue Per Share: double (nullable = true)\n |-- Market Cap: double (nullable = true)\n |-- Enterprise Value: double (nullable = true)\n |-- Forward P/E: double (nullable = true)\n |-- PEG Ratio: double (nullable = true)\n |-- Enterprise Value/Revenue: double (nullable = true)\n |-- Enterprise Value/EBITDA: double (nullable = true)\n |-- Revenue: double (nullable = true)\n |-- Gross Profit: double (nullable = true)\n |-- EBITDA: double (nullable = true)\n |-- Net Income Avl to Common : double (nullable = true)\n |-- Diluted EPS: double (nullable = true)\n |-- Earnings Growth: double (nullable = true)\n |-- Revenue Growth: double (nullable = true)\n |-- Total Cash: double (nullable = true)\n |-- Total Cash Per Share: double (nullable = true)\n |-- Total Debt: double (nullable = true)\n |-- Current Ratio: double (nullable = true)\n |-- Book Value Per Share: double (nullable = true)\n |-- Cash Flow: double (nullable = true)\n |-- Beta: double (nullable = true)\n\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["print(\"Key stats dataframe format:\")\n","key_stats_df.printSchema()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"578db780-5427-4c43-8252-682b1a10c6d0","showTitle":false,"title":""}},"source":["### Utility functions\n","\n","Here are a some utility functions that are used across the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ebf7570c-7796-4442-9a41-c7a8e9069d60","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def prices_df_nan_summary(prices_dfs: List[pyspark.sql.DataFrame], names: List[str]) -> pd.DataFrame:\n","  ''' Utility function to summarize columns that have missing values. '''\n","  nan_dfs = []\n","  for prices_df, name in tqdm(zip(prices_dfs, names), total=len(prices_dfs), desc='Generating prices summary ...'):\n","    nan_absolute = prices_df.agg(*[F.count(F.when(F.isnull(c), c)).alias(c) for c in prices_df.columns]).first()\n","    if any(nan_absolute):\n","      # Simple conversion from Pyspark row -> Python set of values\n","      values = set(nan_absolute.asDict().values()).difference({0})\n","      # Either we don't have values for that row, or we have all of them (but Date which is non-nullable)\n","      # Values contains the no. of NaN values and 0 in correspondance of the Date column\n","      assert len(values) == 1\n","      nan_count = values.pop()\n","      nan_dfs.append((name, round(100*nan_count/prices_df.count(), 3), nan_count))\n","\n","  return pd.DataFrame(nan_dfs, columns=['Stock name', 'Missing data (%)', 'Count'])\n","\n","def remove_trailing_nan(df: pyspark.sql.DataFrame, ticker: str, col: str = 'Low') -> pyspark.sql.DataFrame:\n","  '''\n","    A trick to detect if the input DataFrame ends with a contiguous collection of NaN rows, returns the dataframe without them.\n","  '''\n","  # The total number of rows of the dataframe\n","  df_length = df.count()\n","  \n","  # Sort the input dataframe and add a new column to keep track of the relative position of each row\n","  df_sorted_id = df.sort('Date').withColumn('id', F.row_number().over(Window.orderBy('Date')))\n","  \n","  # Tricky part here: create a new column called 'cumsum' that will store the progressive number of consecutive NaN in our dataset.\n","  # Let's break it into smaller parts:\n","  # 1) create an index generator that will partition by 'Low' values [(...,Null...), (...,value1,...), (...,value2...), ... (...)] and within rows order by date\n","  # Example:\n","  ## |2019-06-05|null|null|  null|null| null|          null|624|        21|\n","  ## |2019-06-06|null|null|  null|null| null|          null|625|        22|\n","  ## |2019-06-07|null|null|  null|null| null|          null|626|        23| <- last column is the cumulative sum (i.e. the number of consecutive NaN)\n","  ## ...\n","  ## |2019-05-09|23.2|11.5|   7.3|4.2|  16.2|          29.1|1  |         0|\n","  ## |2019-05-10|23.2|11.5|   7.3|4.2|  16.2|          29.1|2  |         0|\n","  # 2) assign to each row a progressive index starting from 1 if it has null in correspondance of Low, zero otherwise\n","  # 3) store these values into a new column called cumsum (i.e. it behaves like pandas cumsum)\n","  # 4) at the end, the row whose ID corresponds to the length of the dataframe will contain at column 'cumsum' the no. of trailing NaN values.\n","  cumsum_df = df_sorted_id.withColumn('cumsum', F.when(F.isnull(df_sorted_id.Low), F.row_number().over(Window.partitionBy('Low').orderBy('Date'))).otherwise(0))\n","\n","  # Retrieve the \"last\" row and read the value of cumsum\n","  end_idx = cumsum_df.where(cumsum_df['id'] == df_length).first().cumsum\n","  \n","  # Retain rows whose index is lower len(df) - end_idx + 1 (i.e. cut trailing NaN values)\n","  return df_sorted_id.where(df_sorted_id['id'] <= df_length-end_idx+1)\n","\n","\n","def merge_prices_fundamentals(\n","    prices_dfs: List[pyspark.sql.DataFrame],\n","    key_stats_df: pyspark.sql.DataFrame,\n","    dfs_names: List[str],\n","    drop_cols: List[str] = ['Date', 'Ticker', 'Price']\n","    ) -> List[pyspark.sql.DataFrame]:\n","  # Define the target list of dataframes\n","  prices_dfs_new = []\n","  for ticker in tqdm(key_stats_df.select('Ticker').distinct().collect(), desc='Merging the datasets ...'):\n","    ticker = ticker[0]\n","    # Consider only stocks for which we have fundamental data\n","    if ticker.upper() not in dfs_names: continue\n","      \n","    # The prices dataframe associated to the current ticker\n","    prices_df_idx = dfs_names.index(ticker.upper())\n","    prices_df = prices_dfs[prices_df_idx]\n","    \n","    # Retrieve financial reports for the current ticker\n","    ticker_df = key_stats_df.filter(F.col('Ticker') == ticker)\n","    \n","    # Perform an inner join between the two dataframes, we first take all reports\n","    # whose date is at most the prices date (then we will take the latest available)\n","    joined_df = ticker_df.join(prices_df, ticker_df.Date <= prices_df.Date, how='inner')\n","    \n","    # Let's break this into smaller parts:\n","    # 1) Add a new column for the date latest available report for that stock. To do that\n","    #    we partition over **prices** Date using the Window object, and the the max over\n","    #    **ticker** Date.\n","    # 2) Retain those columns whose financial report date is the same as in ticker_df_max_date\n","    # 3) Finally drop the **ticker** Date and other unused columns\n","    joined_df = joined_df.withColumn('ticker_df_max_date', F.max(ticker_df['Date'])\\\n","                      .over(Window.partitionBy(prices_df['Date'])))\\\n","                      .where(ticker_df['Date'] == F.col('ticker_df_max_date'))\\\n","                      .drop('ticker_df_max_date', 'High', 'Low', 'Open', 'Close')\\\n","                      .drop(ticker_df['Date'])\\\n","                      \n","    # Return the usual list of dataframes\n","    prices_dfs_new.append(joined_df)\n","    \n","  return prices_dfs_new\n","    \n","                \n","def fill_missing_days(\n","  aggregate_dfs: List[pyspark.sql.DataFrame],\n","  remove_weekends: bool = True,\n","  end_date: str = '2014-01-01'\n",") -> List[pyspark.sql.DataFrame]:\n","  result_dfs = []\n","  \n","  @udf(\"boolean\")\n","  def is_weekday(date: datetime) -> bool:\n","      ''' Returns true if the provided date corresponds to a weekday. '''\n","      return date.weekday() < 5\n","    \n","  for df in tqdm(aggregate_dfs, desc='Filling missing days ...'):\n","    # In this case we apply the following steps in order to apply fill-forward to our dataset:\n","    # 1) define a Window object the will over the dataset sorted with decreasing dates\n","    # 2) compute the difference in days between consecutive rows (11/05/2021 - 07/05/2021 ==> 4 days)\n","    #    and store it in a column named diff\n","    # 3) compute an increasing index in the column seq that is the days gap we need to fill\n","    # Example:\n","    # |Date      | Adjusted close   |diff|seq|new_date  |\n","    # |2021-04-09|116.80999755859375|   3|  1|2021-04-11|\n","    # |2021-04-09|116.80999755859375|   3|  2|2021-04-10|\n","    # |2021-04-09|116.80999755859375|   3|  3|2021-04-09|\n","    # 4) create a new column new_date as the result of the computation F.col('Date') + F.col('diff') - F.col('seq')\n","    # 5) only retain rows whose date is before the provided end_date\n","    #\n","    # N.B. if we want to bfill the values we should instead consider the dataframes with dates in ascending order,\n","    # and subtract the value of F.col('diff') instead of summing it. Other parts of the code stays the same.\n","    df = df\\\n","    .withColumn('diff', F.datediff(F.lag(F.col('Date'),1).over(Window.orderBy(F.desc('Date'))), F.col('Date')))\\\n","    .withColumn('seq', F.explode(F.sequence(F.lit(1), F.col(\"diff\"))))\\\n","    .withColumn('new_date', (F.col('Date') + F.col('diff') - F.col('seq')))\\\n","    .where(F.col('new_date') < pd.Timestamp(end_date))\n","    \n","    # If specified, remove weekends that are most likely created by us as synthetic data with the ffill technique\n","    if remove_weekends:\n","      df = df.where(is_weekday(F.col('new_date')))\n","      \n","    # Drop unecessary columns\n","    df = df.drop('Date', 'diff', 'seq').withColumnRenamed('new_date', 'Date')\n","    \n","    # If the dataframe is actually non-empty, add it to the list of resulting dataframes\n","    if df.count() > 0:\n","      result_dfs.append(df)\n","  return result_dfs\n","\n","def missing_values_summary(df):\n","  ''' Returns a utility summary to view missing values in our dataframe. '''\n","  n = df.count()\n","  \n","  def to_percentage(x: pyspark.sql.column.Column, n: int) -> int:\n","    ''' Utility function to compute the amount of missing values as a percentage of the original dataframe. '''\n","    return F.round(100 * x / n, 3)\n","  \n","  # Aggregate using the count function over null values, and return a view over the obtained (single row) dataframe\n","  return df.agg(*[to_percentage(F.count(F.when(F.isnull(c), c)), n).alias(c) for c in df.columns]).first()\n","\n","def scale_features(dfs: List[pyspark.sql.DataFrame]):\n","    ''' Scales the numerical features to unit variance. '''\n","    \n","    scalable_features = ['DE Ratio', 'Trailing P/E', 'Price/Sales', 'Price/Book',\n","       'Profit Margin', 'Operating Margin', 'Return on Assets',\n","       'Return on Equity', 'Revenue Per Share', 'Market Cap',\n","       'Enterprise Value', 'PEG Ratio', 'Enterprise Value/Revenue',\n","       'Enterprise Value/EBITDA', 'Revenue', 'Gross Profit', 'EBITDA',\n","       'Net Income Avl to Common ', 'Diluted EPS', 'Earnings Growth',\n","       'Revenue Growth', 'Total Cash', 'Total Cash Per Share', 'Total Debt',\n","       'Current Ratio', 'Book Value Per Share', 'Cash Flow', 'Beta', 'Volume',\n","       'Adjusted Close', 'SMA', 'RSI']\n","    \n","\n","    # Aggregating dfs in a single one\n","    aggregate_df = dfs[0]\n","    for df in tqdm(dfs[1:], desc='Performing dfs union ...'):\n","      aggregate_df = aggregate_df.union(df)\n","      \n","    # Computing means and stds for every column of the aggregate df\n","    mean = aggregate_df.select(\n","      [F.mean(F.col(column)) for column in scalable_features]\n","    ).collect()[0]\n","    \n","    std = aggregate_df.select(\n","      [F.stddev(F.col(column)) for column in scalable_features]\n","    ).collect()[0]\n","        \n","    for i in tqdm(range(len(dfs)), desc='Scaling numerical features ...'):\n","      # Scaling scalable features\n","      for j, feature in enumerate(scalable_features):\n","        dfs[i] = dfs[i].withColumn(feature, (dfs[i][feature]-mean[j])/std[j])\n","        #dfs[i] = dfs[i].withColumn(feature, (dfs[i][feature]-mean[feature])/std[feature])\n","    return mean[-3], std[-3]\n","        \n","def impute_missing_values(\n","  prices_dfs_new: List[pyspark.sql.DataFrame],\n","  key_stats_df: pyspark.sql.DataFrame\n",") -> Tuple[List[pyspark.sql.DataFrame], pyspark.sql.DataFrame]:\n","  # define the window\n","  window = Window.orderBy('Date').rowsBetween(Window.unboundedPreceding, 0)\n","\n","  # Forward filling values \n","  # (ref. https://stackoverflow.com/questions/38131982/forward-fill-missing-values-in-spark-python/50422240#50422240)\n","  for i in range(len(prices_dfs_new)):\n","    for col_name in prices_dfs_new[i].schema.names:\n","      col = F.last(prices_dfs_new[i][col_name], ignorenulls=True).over(window)\n","      prices_dfs_new[i] = prices_dfs_new[i].withColumn(col_name, col)\n","\n","  # In this case this dataframe contains financial reports that may contain NaN values either because that\n","  # metric was not available at that time OR because it was monitoring an initial stage of a company growth.\n","  # What we do is to apply the classic fast-forward, and fill initial missing values with zeroes.\n","  # Please note: we also discard the 'Forward P/E' column since the imputation here would introduce too much noise.\n","  key_stats_df_new = key_stats_df.drop('Forward P/E')\n","  for col_name in key_stats_df_new.schema.names:\n","      col = F.last(key_stats_df_new[col_name], ignorenulls=True).over(window)\n","      key_stats_df_new = key_stats_df_new.withColumn(col_name, col)\n","  key_stats_df_new = key_stats_df_new.fillna(0.)\n","  \n","  return prices_dfs_new, key_stats_df_new\n","\n","def from_dfs(\n","  dfs: List[pyspark.sql.DataFrame],\n","  window_size: int = 3,\n","  target_col: str = 'Adjusted Close',\n","  val_date: str = '2012-01-01',\n","  test_date: str = '2013-01-01'\n","):\n","  # Split the dataset into train/val/test using these dates as boundaries\n","  val_begin = pd.Timestamp(val_date)\n","  test_begin = pd.Timestamp(test_date)\n","  \n","  def should_drop(col: pyspark.sql.column, idx: int) -> bool:\n","    ''' Returns whether the provided column should be dropped. '''\n","    return (col.endswith(f'_{idx}') or 'Ticker' in col or 'Date' in col or 'id' in col)\\\n","              and col != f'Adjusted Close_{idx}'\n","\n","  train_split, val_split, test_split = [], [], []\n","  \n","  for df in tqdm(dfs, desc='Loading dataset from dfs ...'):\n","    # Create an ID for each row based on dates\n","    df = df\\\n","      .sort('Date')\\\n","      .withColumn('id', F.row_number().over(Window.orderBy('Date')))\n","    \n","    # Split the stock dataset into train/dev/test splits\n","    train_df = df.where(F.col('Date') < val_begin)\n","    val_df = df.where(F.col('Date').between(val_begin, test_begin-pd.Timedelta(days=1)))\n","    test_df = df.where(F.col('Date') >= test_begin)\n","    \n","    # Add a unique identifier for each column (e.g. Date -> Date_0, Date_1, etc ...)\n","    train_df = prepend_columns_index(train_df)\n","    val_df = prepend_columns_index(val_df)\n","    test_df = prepend_columns_index(test_df)\n","    \n","    # Convert each split from nxm -> nxm*(w+1), where w is the size of the window\n","    train_df = create_window_indexes(train_df, window_size)\n","    val_df = create_window_indexes(val_df, window_size)\n","    test_df = create_window_indexes(test_df, window_size)\n","    \n","    # Collect the processed datasets\n","    train_split.append(train_df)\n","    val_split.append(val_df)\n","    test_split.append(test_df)\n","  \n","  # Initialize the final datasets using the obtained df schema\n","  train_ds = spark.createDataFrame([], train_split[0].schema)\n","  val_ds = spark.createDataFrame([], val_split[0].schema)\n","  test_ds = spark.createDataFrame([], test_split[0].schema)\n","  \n","  # Collect all the dataframes into a single df for each data split\n","  for df in train_split: train_ds = train_ds.union(df)\n","  for df in val_split: val_ds = val_ds.union(df)\n","  for df in test_split: test_ds = test_ds.union(df)\n","  \n","  # Drop unused columns for the target variable and rename the target column as 'y'\n","  drop_cols = [col for col in test_ds.columns if should_drop(col, window_size)]\n","  train_ds = train_ds.drop(*drop_cols).withColumnRenamed(f'Adjusted Close_{window_size}', 'y')\n","  val_ds = val_ds.drop(*drop_cols).withColumnRenamed(f'Adjusted Close_{window_size}', 'y')\n","  test_ds = test_ds.drop(*drop_cols).withColumnRenamed(f'Adjusted Close_{window_size}', 'y')\n","  \n","  # Rename columns to make them compatible with Apache Parquet format\n","  target_cols = list(map(lambda x: '_'.join(re.split(' |/', x)), train_ds.columns))\n","  for c, target in zip(train_ds.columns, target_cols): train_ds = train_ds.withColumnRenamed(c, target)\n","  for c, target in zip(val_ds.columns, target_cols): val_ds = val_ds.withColumnRenamed(c, target)\n","  for c, target in zip(test_ds.columns, target_cols): test_ds = test_ds.withColumnRenamed(c, target)\n","    \n","  # Compute the set of unique features in each split\n","  features = list(set(c[:-2] for c in train_ds.columns if len(c) >= 2))\n","\n","  # Convert each split to have, at each timestep (column), the collection of features at that timestep\n","  for timestep in tqdm(range(window_size), desc='Creating timesteps ...'):\n","    # Consider all the features in this timestep\n","    current_cols = list(map(lambda c: f'{c}_{timestep}', features))\n","    # Convert (price_0, sma_0, market_cap_0) -> 0: (price|sma|market_cap) \n","    train_ds = train_ds.withColumn(f'x_{timestep}', F.array(current_cols)).drop(*current_cols)\n","    val_ds = val_ds.withColumn(f'x_{timestep}', F.array(current_cols)).drop(*current_cols)\n","    test_ds = test_ds.withColumn(f'x_{timestep}', F.array(current_cols)).drop(*current_cols)\n","  \n","  return train_ds, val_ds, test_ds, features\n","\n","def prepend_columns_index(\n","  df: pyspark.sql.DataFrame,\n","  idx: int = 0\n",") -> pyspark.sql.DataFrame:\n","  ''' Utility function that returns a new dataframe where columns are preceeded by [idx]. '''\n","  for col in df.schema.names:\n","    # If this is not the first time this function is applied, simply edit the previous identifier\n","    new_col = f'{col}_{idx}' if idx == 0 else f'{col[:-2]}_{idx}'\n","    df = df.withColumnRenamed(col, new_col)\n","  return df\n","\n","def create_window_indexes(df: pyspark.sql.DataFrame, window_size: int) -> pyspark.sql.DataFrame:\n","  ''' Apply self join multiple times to the input dataframe, return a new one of size (df.count(), len(df.columns)*window_size) '''\n","  joined_df = df\n","  for win_idx in range(1, window_size+1):\n","    joined_df = joined_df.join(\n","      prepend_columns_index(df, win_idx),\n","      F.col('id_0') == F.col(f'id_{win_idx}') - win_idx\n","    )\n","  return joined_df\n","\n","def collate_fn(\n","  time_steps: List[str],\n","  batch: List[Dict[str, np.array]]\n",") -> Tuple[torch.Tensor, torch.Tensor]:\n","  ''' Returns (x, y) pairs where x is a vector of shape: (window_size, feature_size). '''\n","  X = np.asarray([np.stack(itemgetter(*time_steps)(d)) for d in batch])\n","  Y = np.asarray([d['y'] for d in batch])\n","  return torch.from_numpy(X).double(), torch.from_numpy(Y).double()\n","\n","def data_loader_fn(window_size: int, **kwargs) -> DataLoader:\n","  ''' Override default behaviour and return a petastorm.Dataset with custom collate function. '''\n","  time_steps = [f'x_{time_step}' for time_step in range(window_size)]\n","  return DataLoader(**kwargs, collate_fn=partial(collate_fn, time_steps))\n","\n","def r2_score(y_hat: np.ndarray, y: np.ndarray) -> float:\n","  ''' Computes the R Squared coefficient between y_hat and y. '''\n","  rss = torch.sum((y - y_hat) ** 2)\n","  tss = torch.sum((y - y.mean()) ** 2)\n","  return 1 - rss / tss\n","\n","def adjusted_r2_score(y_hat: np.ndarray, y: np.ndarray, p: int) -> float:\n","  ''' Computes the Adjusted R Squared coefficient between y_hat and y. '''\n","  rss = torch.sum((y - y_hat) ** 2)\n","  tss = torch.sum((y - y.mean()) ** 2)\n","  df_e = y.shape[0] - p - 1\n","  df_t = y.shape[0] - 1\n","  adj_coef = df_t/df_e if df_t/df_e > 0 else 1  # sample size < features\n","  return 1 - (rss / tss) * adj_coef\n","  \n","\n","def plot_loss_history(loss_history):\n","  ''' Plots the loss history. '''\n","  plt.plot(loss_history['train_history'], label='Train loss')\n","  plt.plot(loss_history['valid_history'], label='Valid loss')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Loss')\n","  plt.legend(loc=\"upper right\")\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"707b227e-4c9c-4943-b14a-db5253fec795","showTitle":false,"title":""}},"source":["#### Technical indicators"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"de4a5d87-ba54-4970-8ecb-54e9ad305164","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def add_sma(dfs: List[pyspark.sql.DataFrame], period: int = 10) -> None:\n","    ''' Computes the Simple Moving Average from a given dataframe. '''\n","    # Window of previous [period] days.\n","    w = (Window().orderBy(F.col(\"timestamp\").cast('long')).rangeBetween(-period*86400, 0))\n","    for i in tqdm(range(len(dfs)), desc='Adding SMA ...'):\n","        # Adding temporary timestamp column\n","        dfs[i] = dfs[i].withColumn('timestamp', dfs[i].Date.cast('timestamp'))\n","        # Computing moving average over the window\n","        dfs[i] = dfs[i].withColumn(\n","          \"SMA\",\n","          F.avg(\"Adjusted Close\").over(w)\n","        )\n","        # Dropping temporary columns\n","        dfs[i] = dfs[i].drop(\"timestamp\")\n","\n","        \n","def add_rsi(dfs: List[pyspark.sql.DataFrame], period: int = 14) -> None:\n","    ''' Computes the Relative Strength Index from a given dataframe. \n","        Formula available at https://en.wikipedia.org/wiki/Relative_strength_index.\n","        Also adds overbought and oversold when the RSI index hits 70 or 30.'''\n","    for j in tqdm(range(len(dfs)), desc='Adding RSI ...'):\n","        \n","        #dateCol = dfs[j][\"Date\"]\n","        w = Window.orderBy(\"Date\")\n","\n","        # Lagging the adjusted close of one row (to have the price of the previous day)\n","        dfs[j] = dfs[j].withColumn(\"prev_adj_close\", F.lag(dfs[j][\"Adjusted Close\"]).over(w))\n","        # Computing day gain and day loss\n","        dfs[j] = dfs[j].withColumn(\"current_gain\", F.when(dfs[j][\"Adjusted Close\"] >= dfs[j][\"prev_adj_close\"],\n","                                                          dfs[j][\"Adjusted Close\"] - dfs[j][\"prev_adj_close\"])\n","                                                          .otherwise(0.0))\n","        dfs[j] = dfs[j].withColumn(\"current_loss\", F.when(dfs[j][\"prev_adj_close\"] >= dfs[j][\"Adjusted Close\"],\n","                                                         dfs[j][\"prev_adj_close\"] - dfs[j][\"Adjusted Close\"])\n","                                                         .otherwise(0.0))\n","        \n","        # Adding temporary timestamp column\n","        dfs[j] = dfs[j].withColumn('timestamp', dfs[j].Date.cast('timestamp'))\n","        # Window of previous [period] days.\n","        w = (Window().orderBy(F.col(\"timestamp\").cast('long')).rangeBetween(-period*86400, 0))\n","        # Computing moving averages of day gain and day loss over the window\n","        dfs[j] = dfs[j].withColumn(\n","          \"smmau\",\n","          F.avg(\"current_gain\").over(w)\n","        )\n","        \n","        dfs[j] = dfs[j].withColumn(\n","          \"smmad\",\n","          F.avg(\"current_loss\").over(w)\n","        )\n","        \n","        # Computing RSI given the moving averages\n","        dfs[j] = dfs[j].withColumn(\"RSI\", F.when(dfs[j][\"smmau\"] == 0.0, 50.0).when(dfs[j][\"smmad\"] == 0.0, 50.0).otherwise(100 - (100/(1+(dfs[j][\"smmau\"]/dfs[j][\"smmad\"])))))            \n","        # Adding overbought and oversold signals when the RSI goes above 70 and below 30\n","        dfs[j] = dfs[j].withColumn(\"Overbought\", F.when(dfs[j][\"RSI\"] >= 70, 1.0).otherwise(0.0))\n","        dfs[j] = dfs[j].withColumn(\"Oversold\", F.when(dfs[j][\"RSI\"] <= 30, 1.0).otherwise(0.0))\n","        \n","        # Dropping temporary columns\n","        dfs[j] = dfs[j].drop(\"prev_adj_close\", \"current_gain\", \"current_loss\", \"smmau\", \"smmad\", \"timestamp\")\n","        \n","def fit_evaluate(\n","  model_name: str,\n","  window_size: int,\n","  train_df: pyspark.sql.DataFrame,\n","  test_df: pyspark.sql.DataFrame\n",") -> None:\n","  ''' Fits a model on train data and evaluate on test data. '''\n","  features = [f'x_{time_step}' for time_step in range(window_size)]\n","  train_df = train_df.withColumn('x', F.concat(*features)).drop(*features)\n","  test_df = test_df.withColumn('x', F.concat(*features)).drop(*features)\n","  \n","  list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n","  train_df_x = list_to_vector_udf(train_df['x']).alias('x')\n","  test_df_x = list_to_vector_udf(test_df['x']).alias('x')\n","  \n","  train_df = train_df.select(train_df_x, train_df['y'])\n","  test_df = test_df.select(test_df_x, test_df['y'])\n","  if model_name == 'RandomForestRegressor':\n","    _model = RandomForestRegressor(featuresCol='x', labelCol='y')\n","  elif model_name == 'DecisionTreeRegressor':\n","    _model = DecisionTreeRegressor(featuresCol='x', labelCol='y')\n","\n","  model = _model.fit(train_df)\n","  \n","  predicted_values = model.transform(test_df)\n","  evaluator = RegressionEvaluator(labelCol=\"y\", predictionCol=\"prediction\", metricName=\"r2\")\n","  r2 = evaluator.setMetricName('r2').evaluate(predicted_values)\n","  mse = evaluator.setMetricName('mse').evaluate(predicted_values)\n","  \n","  print(f'Performance with {model_name}')\n","  print('R2: {:.3f}, MSE: {:.3f}'.format(r2, mse))\n","  \n","  return predicted_values\n","\n","def evaluate_trading_strategy_ps(\n","  predicted_values: pyspark.sql.DataFrame,\n","  features: List[str],\n","  hparams,\n","  price_std: float,\n","  price_mean: float,\n","  target_feature: str = 'Adjusted_Close'\n",") -> None:\n","  ''' Evaluate the trading strategy of PySpark models given their predictions. '''\n","  target_idx = features.index(target_feature)\n","  preds = predicted_values.collect()\n","  X, y, y_pred = map(np.array, zip(*preds))\n","  X = X.reshape(-1, hparams.window_size, hparams.input_dim)\n","\n","  pnl = 0.0\n","  correct = 0\n","  tries = 0\n","  X[:, :, target_idx] = X[:, :, target_idx] * price_std + price_mean\n","  y_pred = y_pred * price_std + price_mean\n","  y = y * price_std + price_mean\n","  for i in range(X.shape[0]):\n","    # Increase predicted\n","    current_price = X[i, -1, target_idx]\n","    ret = 0.0\n","    if current_price < y_pred[i]:\n","      # Adding long profit / loss\n","      ret = (y[i] - current_price) / current_price\n","\n","      pnl += ret\n","      # Counting correct operations \n","      tries += 1\n","      correct += 1 if ret >= 0 else 0\n","\n","  print(f\"1-year profit: {(pnl.item())*100}%\")\n","  print(f\"Operation accuracy: {(correct/tries)*100}%\")\n","\n","@torch.no_grad()\n","def evaluate_trading_strategy(\n","  model: nn.Module,\n","  test_dataloader: DataLoader,\n","  mean: float,\n","  std: float,\n","  stock_num: int,\n","  target_idx: int = -1\n",") -> None:\n","  pnl = 0.0\n","  tries = 0\n","  correct = 0\n","  for X, y in test_dataloader:\n","    y_pred = model(X.float())\n","    # Re-adding mean to prices to make them positive\n","    X[:, :, target_idx] = X[:, :, target_idx] * std + mean\n","    y_pred = y_pred * std + mean\n","    y = y * std + mean\n","    for i in range(X.shape[0]):\n","      # Increase predicted\n","      current_price = X[i, -1, target_idx]\n","      ret = 0.0\n","      if current_price < y_pred[i]:\n","        # Adding long profit / loss\n","        ret = (y[i] - current_price) / current_price\n","        \n","        pnl += ret\n","        # Counting correct operations \n","        tries += 1\n","        correct += 1 if ret >= 0 else 0\n","\n","  print(f\"1-year profit: {(pnl)*100}%\")\n","  print(f\"Operation accuracy: {(correct/tries)*100}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9a201af9-db34-4924-81d9-4982833e72e1","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Overview of the missing values in the key_stats dataframe\n","\n","Out[9]: Row(Date=0.0, Ticker=0.0, Price=2.326, DE Ratio=17.674, Trailing P/E=0.93, Price/Sales=0.0, Price/Book=0.465, Profit Margin=0.0, Operating Margin=0.0, Return on Assets=1.86, Return on Equity=1.86, Revenue Per Share=0.0, Market Cap=0.0, Enterprise Value=1.395, Forward P/E=40.465, PEG Ratio=0.0, Enterprise Value/Revenue=1.395, Enterprise Value/EBITDA=3.256, Revenue=1.395, Gross Profit=0.465, EBITDA=2.791, Net Income Avl to Common =0.0, Diluted EPS=0.465, Earnings Growth=5.116, Revenue Growth=3.256, Total Cash=0.465, Total Cash Per Share=0.465, Total Debt=8.372, Current Ratio=9.302, Book Value Per Share=0.93, Cash Flow=10.233, Beta=2.791)</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Overview of the missing values in the key_stats dataframe\n\nOut[9]: Row(Date=0.0, Ticker=0.0, Price=2.326, DE Ratio=17.674, Trailing P/E=0.93, Price/Sales=0.0, Price/Book=0.465, Profit Margin=0.0, Operating Margin=0.0, Return on Assets=1.86, Return on Equity=1.86, Revenue Per Share=0.0, Market Cap=0.0, Enterprise Value=1.395, Forward P/E=40.465, PEG Ratio=0.0, Enterprise Value/Revenue=1.395, Enterprise Value/EBITDA=3.256, Revenue=1.395, Gross Profit=0.465, EBITDA=2.791, Net Income Avl to Common =0.0, Diluted EPS=0.465, Earnings Growth=5.116, Revenue Growth=3.256, Total Cash=0.465, Total Cash Per Share=0.465, Total Debt=8.372, Current Ratio=9.302, Book Value Per Share=0.93, Cash Flow=10.233, Beta=2.791)</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["print(\"Overview of the missing values in the key_stats dataframe\\n\")\n","key_stats_summary = missing_values_summary(key_stats_df)\n","key_stats_summary"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"db3487d2-05a9-43f2-a9d0-1feb1ece31f9","showTitle":false,"title":""}},"source":["### Missing values imputation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summary = prices_df_nan_summary(prices_dfs, dfs_names)\n","px.bar(summary, x='Stock name', y='Missing data (%)', hover_data=['Count'], title=\"Stock price dataset before preprocessing (only columns with missing values are displayed)\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"46156c58-76a8-40c8-88a3-b4bf0a800dd5","showTitle":false,"title":""}},"source":["For most of the above stocks with missing values, we noticed that they indeed exist up to a given time and after that no more data is available. It may due to a business failure, hence no more stocks will be exchanged from that moment on."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Clear our input data from training NaN values\n","prices_dfs_new = [remove_trailing_nan(df,name) for df,name in tqdm(zip(prices_dfs, dfs_names), total=len(prices_dfs), desc='Removing trailing NaN values ...')]\n","\n","# Remove INTH stock from our dataset since it contains many inactivity periods\n","if 'INTH' in dfs_names:\n","  inth_idx = dfs_names.index('INTH')\n","  del dfs_names[inth_idx]\n","  del prices_dfs_new[inth_idx]\n","\n","summary = prices_df_nan_summary(prices_dfs_new, dfs_names)\n","px.bar(summary, x='Stock name', y='Missing data (%)', hover_data=['Count'], title=\"Stock price dataset after preprocessing (only columns with missing values are displayed)\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b219a552-fd13-453f-826c-71fb94bfd81f","showTitle":false,"title":""}},"source":["At this point we use the fast forward imputation technique to fill-in missing values. Please note that in this case missing values are mostly due to holidays or periods when stocks are not exchanged."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c47e3765-14b1-4228-a3b8-51985f53cd14","showTitle":false,"title":""}},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b0fb8282-de11-4c4a-86ba-39b556d9b988","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["@dataclasses.dataclass\n","class HParams:\n","  input_dim: int = -1  # Denoted by data transformations\n","  window_size: int = 10\n","  batch_size: int = 1024\n","  features: int = -1  # Denoted by data transformations\n","  hidden_dim: int = 128\n","  num_layers: int = 1\n","  dropout: int = 0.5\n","  lr: int = 0.1\n","    \n","hparams = HParams()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3595c39e-c759-41d4-a927-ab75e577b16e","showTitle":false,"title":""}},"source":["## Building our new dataset\n","\n","We impute missing values, merge the two datasets, fill missing days and compute additional features (e.g. RSI)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Impute missing values in the prices dataset (i.e. fast-forward last valid values)\n","prices_dfs_new, key_stats_df_new = impute_missing_values(prices_dfs_new, key_stats_df)\n","\n","# Merge the stock price dataset with fundamental data of the relative company\n","aggregate_dfs = merge_prices_fundamentals(prices_dfs_new, key_stats_df_new, dfs_names)\n","\n","# Fill gaps from the original dataset\n","dfs = fill_missing_days(aggregate_dfs)\n","                  \n","# Add SMA indicator to each dataframe\n","add_sma(dfs)\n","\n","# Add RSI indicator to each dataframe\n","add_rsi(dfs)\n","\n","# Scale numerical features\n","price_mean, price_std = scale_features(dfs)\n","\n","# Create train/dev/test splits\n","train_ds, val_ds, test_ds, features = from_dfs(dfs, window_size=hparams.window_size)\n","hparams.input_dim = len(features)\n","hparams.features = features"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b15deec7-b5a2-49db-a08d-94bd3145ed1c","showTitle":false,"title":""}},"source":["##LSTM definition\n","\n","Here we define the LSTM model, using the PyTorch deep learning framework."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"92ce704a-ec3c-43f9-b503-6fc7bf2aaa65","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["class SimpleLSTM(nn.Module):\n","  def __init__(self, hparams):\n","      super().__init__()\n","      self.hparams = hparams\n","      self.lstm = nn.LSTM(hparams.input_dim, hparams.hidden_dim, num_layers=hparams.num_layers, batch_first=True)\n","      self.fc1 = nn.Linear(hparams.hidden_dim, hparams.hidden_dim//2)\n","      self.fc2 = nn.Linear(hparams.hidden_dim//2, 1)\n","      self.dropout = nn.Dropout(hparams.dropout)\n","\n","  def forward(self, x: np.ndarray):\n","      x, (h,c) = self.lstm(x)\n","      x = torch.relu(self.fc1(x[:,-1]))\n","      x = self.fc2(x)\n","      return x.squeeze()\n","\n","def train(\n","  model: nn.Module,\n","  optimizer: torch.optim.SGD,\n","  train_dataloader: DataLoader,\n","  valid_dataloader: DataLoader,\n","  logger,\n","  epochs: int = 5\n",") -> None:\n","  train_history = []\n","  valid_history = []\n","  mean_r2 = 0.0\n","  mean_adjusted_r2 = 0.0\n","  best_r2 = 0\n","  best_adj_r2 = 0\n","  model_state = model.state_dict().copy()\n","  best_epoch = 0\n","\n","  progress = tqdm(range(epochs))\n","  progress.set_description(f\"Epoch: {0}, train loss: 0, val loss: 0\")\n","  for epoch in progress:\n","      losses = []\n","      r2_ls = []\n","      adjusted_r2_ls = []\n","\n","      for x, y in train_dataloader:\n","          # Zero the gradients to prevent Pytorch from accumulating the gradients\n","          optimizer.zero_grad()\n","          y_hat = model(x.float())\n","\n","          # Compute the loss\n","          loss = loss_fn(y_hat, y.float())\n","          losses.append(loss)\n","\n","          # Compute the gradient of the loss and update model parameters\n","          loss.backward()\n","          optimizer.step()\n","\n","      mean_loss = sum(losses) / len(losses)\n","      train_history.append(mean_loss.item())\n","\n","      losses = []\n","      with torch.no_grad():\n","          for x, y in valid_dataloader:\n","              y_hat = model(x.float())\n","              loss = loss_fn(y_hat, y.float())\n","              losses.append(loss)\n","              r2_ls.append(r2_score(y_hat, y))\n","              adjusted_r2_ls.append(adjusted_r2_score(y_hat, y, x.shape[1] * x.shape[2]))\n","\n","      mean_loss = sum(losses) / len(losses)\n","      valid_history.append(mean_loss.item())\n","      mean_r2 = sum(r2_ls) / len(r2_ls)\n","      mean_adjusted_r2 = sum(adjusted_r2_ls) / len(adjusted_r2_ls)\n","      \n","      if mean_adjusted_r2 > best_adj_r2:\n","          #print(f'\\nCurrent best is {mean_adjusted_r2}')\n","          best_adj_r2 = mean_adjusted_r2\n","          best_r2 = mean_r2\n","          model_state = model.state_dict().copy()\n","          best_epoch = epoch\n","      \n","      logger.log(\n","        {\n","          'train_loss': train_history[-1],\n","          'valid_loss': valid_history[-1],\n","          'r2': mean_r2,\n","          'adjusted_r2': mean_adjusted_r2,\n","        }\n","      )\n","      \n","      progress.set_description(f\"Epoch: {epoch+1}, train loss: {train_history[-1]}, val loss: {valid_history[-1]}\")\n","      \n","  # Select the model with the best overall performances\n","  model.load_state_dict(model_state)\n","  print(f'Saving the parameters of the best model at epoch {best_epoch}')\n","\n","  return {\n","      'train_history': train_history,\n","      'valid_history': valid_history,\n","      'r2': best_r2,\n","      'adj_r2': best_adj_r2\n","  }\n","\n","@torch.no_grad()\n","def evaluate(model, dataloader):\n","    Y = []\n","    Y_pred = []\n","\n","    for x, y in dataloader:\n","        y_hat = model(x.float())\n","        Y.append(y)\n","        Y_pred.append(y_hat)\n","\n","    return {\n","        'y_true': torch.stack(Y).reshape(-1),\n","        'y_pred': torch.stack(Y_pred).reshape(-1).round()\n","    }"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"355d5de2-0b6f-4273-8df9-1cd3ca813f90","showTitle":false,"title":""}},"source":["##LSTM training and evaluation\n","\n","Here we train and evaluate the LSTM model. In order to load PySpark dataframes into PyTorch dataloaders we need to use Petastorm that loads the data in Apache Parquet format and distributes it across the nodes. The evaluation consists of calulating scores such as the Mean Squared Error and the R squared, but it also involves testing the performances of the model in terms of profit calculation. In fact, we backtest our model by estimating the profit over one year of daily operations, according to the model's predictions. We also take into consideration the operation accuracy (i.e. the percentage of times that the model correctly classifies an increase in the price)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the number of epochs to run the model on\n","hparams.epochs = 100\n","hparams.lr = 0.01\n","hparams.dropout = 0.0\n","hparams.momentum = 0.3\n","\n","# Configure the current run\n","run = wandb.init(reinit=True, project='distributed_spf', config=hparams)\n","\n","# Define the loss function and the model's optimizer\n","loss_fn = nn.MSELoss()\n","model = SimpleLSTM(hparams)\n","optimizer = torch.optim.SGD(model.parameters(), lr=hparams.lr, momentum=hparams.momentum)\n","\n","# Materialize spark dataframes to DBFS in parqet format\n","converter_train = make_spark_converter(train_ds)\n","converter_valid = make_spark_converter(val_ds)\n","converter_test = make_spark_converter(test_ds)\n","\n","# Fit the model on training data, and evaluate it on validation split\n","with converter_train.make_torch_dataloader(\n","  batch_size=hparams.batch_size, data_loader_fn=partial(data_loader_fn, hparams.window_size), num_epochs=1\n",") as train_dataloader, converter_valid.make_torch_dataloader(\n","  batch_size=hparams.batch_size, data_loader_fn=partial(data_loader_fn, hparams.window_size), num_epochs=1\n",") as valid_dataloader, converter_test.make_torch_dataloader(\n","  batch_size=hparams.batch_size, data_loader_fn=partial(data_loader_fn, hparams.window_size), num_epochs=1\n",") as test_dataloader:\n","  \n","  train_logs = train(model, optimizer, train_dataloader, valid_dataloader, run, epochs=hparams.epochs)\n","  plot_loss_history(train_logs)\n","  \n","  evaluate_trading_strategy(model, test_dataloader, price_mean, price_std, len(dfs), target_idx=features.index('Adjusted_Close'))\n","\n","# Log the experiments\n","run.finish()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ea6dff9f-d37a-4581-a7a7-6c705b08fb95","showTitle":false,"title":""}},"source":["##Other regressors\n","\n","Here we train and evaluate other simpler regressors that are provided directly by PySpark. This is done for comparison of the more articulated LSTM network with these simpler approaches."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"03eb7a62-7851-4c8d-9bdd-4328e09151d1","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Performance with DecisionTreeRegressor\n","R2: 0.852, MSE: 0.078\n","1-year profit: 35.970558703493936%\n","Operation accuracy: 55.45286506469501%\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Performance with DecisionTreeRegressor\nR2: 0.852, MSE: 0.078\n1-year profit: 35.970558703493936%\nOperation accuracy: 55.45286506469501%\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["tree_predicted_values = fit_evaluate('DecisionTreeRegressor', hparams.window_size, train_ds, test_ds)\n","evaluate_trading_strategy_ps(tree_predicted_values, features, hparams, price_std, price_mean)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0388bb22-41e8-4f62-a3cd-a6bf6f6f2e5f","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Performance with RandomForestRegressor\n","R2: 0.803, MSE: 0.104\n","1-year profit: 51.61961803841207%\n","Operation accuracy: 57.019064124783355%\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Performance with RandomForestRegressor\nR2: 0.803, MSE: 0.104\n1-year profit: 51.61961803841207%\nOperation accuracy: 57.019064124783355%\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["rf_predicted_values = fit_evaluate('RandomForestRegressor', hparams.window_size, train_ds, test_ds)\n","evaluate_trading_strategy_ps(rf_predicted_values, features, hparams, price_std, price_mean)"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"dist_forecasting","notebookOrigID":744412709040545,"widgets":{}},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.7.12 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}
