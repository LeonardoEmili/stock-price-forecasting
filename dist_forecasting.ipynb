{"cells":[{"cell_type":"markdown","source":["## Import required packages"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e862fc71-84e1-4af7-badc-898efadd0bc5"}}},{"cell_type":"code","source":["# Download required packages\n!pip -q install gdown missingno torch\n\n%matplotlib inline\n\nimport pyspark\nfrom pyspark.sql import *\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql.window import Window\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport missingno as msno\nimport torch\nimport torch.nn as nn\nfrom typing import *\nimport datetime\nimport gdown\n\nimport tqdm as tq\ndef tqdm(*args, **kwargs):\n  ''' Small trick to prevent tqdm printing newlines at each step. '''\n  return tq.tqdm(*args, **kwargs, leave=True, position=0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d89c4672-97c1-49d3-9350-6008885da2ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"><span class=\"ansi-yellow-fg\">WARNING: You are using pip version 20.2.4; however, version 21.1.1 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-yellow-fg\">WARNING: You are using pip version 20.2.4; however, version 21.1.1 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Data aquisition\nWe retrieve our datasets and download them to a temporary directory in the driver node."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f76226d3-37ac-40d6-a417-a05ca05a7000"}}},{"cell_type":"code","source":["!rm -rf /tmp/data /tmp/__MACOSX\ngdown.download('https://drive.google.com/uc?id=1ggmDp-AWFzbQReLG0pLpQE_3fO0C0RnM', '/tmp/data.zip', quiet=False)\n!unzip -q /tmp/data.zip -d /tmp/\n!rm /tmp/data.zip"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c1efdeb-4084-4115-9920-536feac86d20"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Downloading...\nFrom: https://drive.google.com/uc?id=1ggmDp-AWFzbQReLG0pLpQE_3fO0C0RnM\nTo: /tmp/data.zip\n\r0.00B [00:00, ?B/s]\r4.72MB [00:00, 39.4MB/s]\r17.3MB [00:00, 37.0MB/s]\r34.1MB [00:00, 57.9MB/s]\r42.5MB [00:00, 57.8MB/s]\r59.2MB [00:01, 48.0MB/s]\r71.9MB [00:01, 56.8MB/s]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Downloading...\nFrom: https://drive.google.com/uc?id=1ggmDp-AWFzbQReLG0pLpQE_3fO0C0RnM\nTo: /tmp/data.zip\n\r0.00B [00:00, ?B/s]\r4.72MB [00:00, 39.4MB/s]\r17.3MB [00:00, 37.0MB/s]\r34.1MB [00:00, 57.9MB/s]\r42.5MB [00:00, 57.8MB/s]\r59.2MB [00:01, 48.0MB/s]\r71.9MB [00:01, 56.8MB/s]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Then we load the datasets to the DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"941ae011-778f-4659-9774-ff7df196b5b1"}}},{"cell_type":"code","source":["dbutils.fs.mv(\"file:/tmp/data\", \"dbfs:/data\", recurse=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"791fea85-d7b5-4432-b530-ddae393cc5d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[4]: True</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs ls /data/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c18efa6b-4546-4cab-9e4b-582119acee64"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/data/.DS_Store",".DS_Store",6148],["dbfs:/data/key_stats_yahoo.csv","key_stats_yahoo.csv",2047081],["dbfs:/data/prices/","prices/",0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/data/.DS_Store</td><td>.DS_Store</td><td>6148</td></tr><tr><td>dbfs:/data/key_stats_yahoo.csv</td><td>key_stats_yahoo.csv</td><td>2047081</td></tr><tr><td>dbfs:/data/prices/</td><td>prices/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs ls /data/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5e7b9cc-713e-4303-a194-15a532c0757b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/data/.DS_Store",".DS_Store",6148],["dbfs:/data/key_stats_yahoo.csv","key_stats_yahoo.csv",2047081],["dbfs:/data/prices/","prices/",0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/data/.DS_Store</td><td>.DS_Store</td><td>6148</td></tr><tr><td>dbfs:/data/key_stats_yahoo.csv</td><td>key_stats_yahoo.csv</td><td>2047081</td></tr><tr><td>dbfs:/data/prices/</td><td>prices/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Dataset loading"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"898c47b5-01c8-4a5c-86c1-58bd6334f04a"}}},{"cell_type":"code","source":["key_stats_df = spark.read.load(\"dbfs:/data/key_stats_yahoo.csv\", \n                           format=\"csv\",\n                           sep=\",\",\n                           inferSchema=\"true\",\n                           header=\"true\"\n                          )\n\n# Drop the first ID column\nkey_stats_df = sc.parallelize(key_stats_df.drop(key_stats_df.columns[0]).head(1005)).toDF()#TODO: remove head(n) (only meant for development)\nkey_stats_df.schema['Date'].nullable = False\n\n# Use legacy format to parse dates\nspark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\nkey_stats_df = key_stats_df.withColumn(\"Date\", F.to_date(key_stats_df[\"Date\"], 'MM/dd/yyyy HH:mm'))\n\n# Cast numerical columns to double\nfor column in key_stats_df.columns[2:]:\n  key_stats_df = key_stats_df.withColumn(column, key_stats_df[column].cast(\"double\"))\n\n# Prices dataframes for every stock #TODO: remove :N (only meant for development)\nprices_files = [f.path for f in dbutils.fs.ls('/data/prices/')[:10] if f.path.endswith('.csv')]\ndfs_names = [f.rsplit('/', 1)[1][:-len('.csv')] for f in prices_files]\nprices_dfs = []\nfor f in tqdm(prices_files, desc='Reading stock price data', total=len(prices_files)):\n  df = spark.read.load(f,\n                       format=\"csv\",\n                       sep=\",\",\n                       inferSchema=\"true\",\n                       header=\"true\"\n                      )\n  df = df.withColumn(\"Date\", F.to_date(df[\"Date\"], 'dd-MM-yyyy'))\n  df.schema['Date'].nullable = False\n  prices_dfs.append(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"832ad8e5-1aed-43f4-8748-ec7a4ced09bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\rReading stock price data:   0%|          | 0/9 [00:00&lt;?, ?it/s]\rReading stock price data:  11%|█         | 1/9 [00:01&lt;00:13,  1.71s/it]\rReading stock price data:  22%|██▏       | 2/9 [00:02&lt;00:09,  1.39s/it]\rReading stock price data:  33%|███▎      | 3/9 [00:03&lt;00:07,  1.26s/it]\rReading stock price data:  44%|████▍     | 4/9 [00:05&lt;00:06,  1.31s/it]\rReading stock price data:  56%|█████▌    | 5/9 [00:06&lt;00:04,  1.22s/it]\rReading stock price data:  67%|██████▋   | 6/9 [00:07&lt;00:03,  1.23s/it]\rReading stock price data:  78%|███████▊  | 7/9 [00:08&lt;00:02,  1.20s/it]\rReading stock price data:  89%|████████▉ | 8/9 [00:09&lt;00:01,  1.12s/it]\rReading stock price data: 100%|██████████| 9/9 [00:10&lt;00:00,  1.05s/it]\rReading stock price data: 100%|██████████| 9/9 [00:10&lt;00:00,  1.18s/it]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\rReading stock price data:   0%|          | 0/9 [00:00&lt;?, ?it/s]\rReading stock price data:  11%|█         | 1/9 [00:01&lt;00:13,  1.71s/it]\rReading stock price data:  22%|██▏       | 2/9 [00:02&lt;00:09,  1.39s/it]\rReading stock price data:  33%|███▎      | 3/9 [00:03&lt;00:07,  1.26s/it]\rReading stock price data:  44%|████▍     | 4/9 [00:05&lt;00:06,  1.31s/it]\rReading stock price data:  56%|█████▌    | 5/9 [00:06&lt;00:04,  1.22s/it]\rReading stock price data:  67%|██████▋   | 6/9 [00:07&lt;00:03,  1.23s/it]\rReading stock price data:  78%|███████▊  | 7/9 [00:08&lt;00:02,  1.20s/it]\rReading stock price data:  89%|████████▉ | 8/9 [00:09&lt;00:01,  1.12s/it]\rReading stock price data: 100%|██████████| 9/9 [00:10&lt;00:00,  1.05s/it]\rReading stock price data: 100%|██████████| 9/9 [00:10&lt;00:00,  1.18s/it]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Dataset analysis"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be1bb73d-1713-4401-9ca9-cef3f47a976f"}}},{"cell_type":"code","source":["print(\"Prices dataframe format:\")\nprices_dfs[0].printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd6040d0-f97c-4ef4-bd0d-3a1a8d4214a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Prices dataframe format:\nroot\n |-- Date: date (nullable = true)\n |-- Low: double (nullable = true)\n |-- Open: double (nullable = true)\n |-- Volume: integer (nullable = true)\n |-- High: double (nullable = true)\n |-- Close: double (nullable = true)\n |-- Adjusted Close: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Prices dataframe format:\nroot\n-- Date: date (nullable = true)\n-- Low: double (nullable = true)\n-- Open: double (nullable = true)\n-- Volume: integer (nullable = true)\n-- High: double (nullable = true)\n-- Close: double (nullable = true)\n-- Adjusted Close: double (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"Key stats dataframe format:\")\nkey_stats_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc56a262-df2a-4a39-ab77-76f55ee2948c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Key stats dataframe format:\nroot\n |-- Date: date (nullable = true)\n |-- Ticker: string (nullable = true)\n |-- Price: double (nullable = true)\n |-- DE Ratio: double (nullable = true)\n |-- Trailing P/E: double (nullable = true)\n |-- Price/Sales: double (nullable = true)\n |-- Price/Book: double (nullable = true)\n |-- Profit Margin: double (nullable = true)\n |-- Operating Margin: double (nullable = true)\n |-- Return on Assets: double (nullable = true)\n |-- Return on Equity: double (nullable = true)\n |-- Revenue Per Share: double (nullable = true)\n |-- Market Cap: double (nullable = true)\n |-- Enterprise Value: double (nullable = true)\n |-- Forward P/E: double (nullable = true)\n |-- PEG Ratio: double (nullable = true)\n |-- Enterprise Value/Revenue: double (nullable = true)\n |-- Enterprise Value/EBITDA: double (nullable = true)\n |-- Revenue: double (nullable = true)\n |-- Gross Profit: double (nullable = true)\n |-- EBITDA: double (nullable = true)\n |-- Net Income Avl to Common : double (nullable = true)\n |-- Diluted EPS: double (nullable = true)\n |-- Earnings Growth: double (nullable = true)\n |-- Revenue Growth: double (nullable = true)\n |-- Total Cash: double (nullable = true)\n |-- Total Cash Per Share: double (nullable = true)\n |-- Total Debt: double (nullable = true)\n |-- Current Ratio: double (nullable = true)\n |-- Book Value Per Share: double (nullable = true)\n |-- Cash Flow: double (nullable = true)\n |-- Beta: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Key stats dataframe format:\nroot\n-- Date: date (nullable = true)\n-- Ticker: string (nullable = true)\n-- Price: double (nullable = true)\n-- DE Ratio: double (nullable = true)\n-- Trailing P/E: double (nullable = true)\n-- Price/Sales: double (nullable = true)\n-- Price/Book: double (nullable = true)\n-- Profit Margin: double (nullable = true)\n-- Operating Margin: double (nullable = true)\n-- Return on Assets: double (nullable = true)\n-- Return on Equity: double (nullable = true)\n-- Revenue Per Share: double (nullable = true)\n-- Market Cap: double (nullable = true)\n-- Enterprise Value: double (nullable = true)\n-- Forward P/E: double (nullable = true)\n-- PEG Ratio: double (nullable = true)\n-- Enterprise Value/Revenue: double (nullable = true)\n-- Enterprise Value/EBITDA: double (nullable = true)\n-- Revenue: double (nullable = true)\n-- Gross Profit: double (nullable = true)\n-- EBITDA: double (nullable = true)\n-- Net Income Avl to Common : double (nullable = true)\n-- Diluted EPS: double (nullable = true)\n-- Earnings Growth: double (nullable = true)\n-- Revenue Growth: double (nullable = true)\n-- Total Cash: double (nullable = true)\n-- Total Cash Per Share: double (nullable = true)\n-- Total Debt: double (nullable = true)\n-- Current Ratio: double (nullable = true)\n-- Book Value Per Share: double (nullable = true)\n-- Cash Flow: double (nullable = true)\n-- Beta: double (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Utility functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"578db780-5427-4c43-8252-682b1a10c6d0"}}},{"cell_type":"code","source":["# TODO: add remaining utility functions\n\ndef prices_df_nan_summary(prices_dfs: List[pyspark.sql.DataFrame], names: List[str]) -> pd.DataFrame:\n  ''' Utility function to summarize columns that have missing values. '''\n  nan_dfs = []\n  for prices_df, name in tqdm(zip(prices_dfs, names), total=len(prices_dfs), desc='Generating prices summary ...'):\n    nan_absolute = prices_df.agg(*[F.count(F.when(F.isnull(c), c)).alias(c) for c in prices_df.columns]).first()\n    if any(nan_absolute):\n      # Simple conversion from Pyspark row -> Python set of values\n      values = set(nan_absolute.asDict().values()).difference({0})\n      # Either we don't have values for that row, or we have all of them (but Date which is non-nullable)\n      # Values contains the no. of NaN values and 0 in correspondance of the Date column\n      assert len(values) == 1\n      nan_count = values.pop()\n      nan_dfs.append((name, round(100*nan_count/prices_df.count(), 3), nan_count))\n\n  return pd.DataFrame(nan_dfs, columns=['Stock name', 'Missing data (%)', 'Count'])\n\ndef remove_trailing_nan(df: pyspark.sql.DataFrame, ticker: str, col: str = 'Low') -> pyspark.sql.DataFrame:\n  '''\n    A trick to detect if the input DataFrame ends with a contiguous collection of NaN rows, returns the dataframe without them.\n  '''\n  # The total number of rows of the dataframe\n  df_length = df.count()\n  \n  # Sort the input dataframe and add a new column to keep track of the relative position of each row\n  df_sorted_id = df.sort('Date').withColumn('id', F.row_number().over(Window.orderBy('Date')))\n  \n  # Tricky part here: create a new column called 'cumsum' that will store the progressive number of consecutive NaN in our dataset.\n  # Let's break it into smaller parts:\n  # 1) create an index generator that will partition by 'Low' values [(...,Null...), (...,value1,...), (...,value2...), ... (...)] and within rows order by date\n  # Example:\n  ## |2019-06-05|null|null|  null|null| null|          null|624|        21|\n  ## |2019-06-06|null|null|  null|null| null|          null|625|        22|\n  ## |2019-06-07|null|null|  null|null| null|          null|626|        23| <- last column is the cumulative sum (i.e. the number of consecutive NaN)\n  ## ...\n  ## |2019-05-09|23.2|11.5|   7.3|4.2|  16.2|          29.1|1  |         0|\n  ## |2019-05-10|23.2|11.5|   7.3|4.2|  16.2|          29.1|2  |         0|\n  # 2) assign to each row a progressive index starting from 1 if it has null in correspondance of Low, zero otherwise\n  # 3) store these values into a new column called cumsum (i.e. it behaves like pandas cumsum)\n  # 4) at the end, the row whose ID corresponds to the length of the dataframe will contain at column 'cumsum' the no. of trailing NaN values.\n  cumsum_df = df_sorted_id.withColumn('cumsum', F.when(F.isnull(df_sorted_id.Low), F.row_number().over(Window.partitionBy('Low').orderBy('Date'))).otherwise(0))\n\n  # Retrieve the \"last\" row and read the value of cumsum\n  end_idx = cumsum_df.where(cumsum_df['id'] == df_length).first().cumsum\n  \n  # Retain rows whose index is lower len(df) - end_idx + 1 (i.e. cut trailing NaN values)\n  return df_sorted_id.where(df_sorted_id['id'] <= df_length-end_idx+1)\n\n\ndef merge_prices_fundamentals(\n    prices_dfs: List[pyspark.sql.DataFrame],\n    key_stats_df: pyspark.sql.DataFrame,\n    dfs_names: List[str],\n    drop_cols: List[str] = ['Date', 'Ticker', 'Price']\n    ) -> List[pyspark.sql.DataFrame]:\n  # Define the target list of dataframes\n  prices_dfs_new = []\n  for ticker in tqdm(key_stats_df.select('Ticker').distinct().collect(), desc='Merging the datasets ...'):\n    ticker = ticker[0]\n    ticker_df = key_stats_df.filter(F.col('Ticker') == ticker).collect()\n    ticker_df = sc.parallelize(ticker_df).toDF()\n    \n    \n    # Consider only stocks for which we have fundamental data\n    if ticker.upper() not in dfs_names: continue\n    \n    # Dropping fundamentals useless columns\n    for col in drop_cols:\n      ticker_df = ticker_df.drop(col)\n      \n    \n    # Consider the relative stock ticker dataframe\n    prices_df_idx = dfs_names.index(ticker.upper())\n    prices_df = prices_dfs[prices_df_idx]\n    # Dropping unused price value columns (we are using adjusted close)\n    for col in ['High', 'Low', 'Open', 'Close']:\n      prices_df = prices_df.drop(col)\n    fundamental_rows = []\n\n\n    # The current financial report\n    ticker_iterator = list(ticker_df.iterrows())\n    ticker_iter_idx = 0\n    \n                \ndef fill_missing_days(aggregate_dfs: List[pyspark.sql.DataFrame], remove_weekends: bool = True, end_year: int = 2013) -> List[pyspark.sql.DataFrame]:\n  pass\n\ndef missing_values_summary(df):\n  ''' Returns a utility summary to view missing values in our dataframe. '''\n  n = df.count()\n  \n  def to_percentage(x: pyspark.sql.column.Column, n: int) -> int:\n    ''' Utility function to compute the amount of missing values as a percentage of the original dataframe. '''\n    return F.round(100 * x / n, 3)\n  \n  # Aggregate using the count function over null values, and return a view over the obtained (single row) dataframe\n  return df.agg(*[to_percentage(F.count(F.when(F.isnull(c), c)), n).alias(c) for c in df.columns]).first()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebf7570c-7796-4442-9a41-c7a8e9069d60"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"Overview of the missing values in the key_stats dataframe\\n\")\nkey_stats_summary = missing_values_summary(key_stats_df)\nkey_stats_summary"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a201af9-db34-4924-81d9-4982833e72e1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Overview of the missing values in the key_stats dataframe\n\nOut[9]: Row(Date=0.0, Ticker=0.0, Price=1.99, DE Ratio=18.607, Trailing P/E=9.652, Price/Sales=0.498, Price/Book=2.189, Profit Margin=1.194, Operating Margin=0.498, Return on Assets=1.99, Return on Equity=2.786, Revenue Per Share=0.398, Market Cap=0.199, Enterprise Value=1.095, Forward P/E=46.269, PEG Ratio=4.776, Enterprise Value/Revenue=1.294, Enterprise Value/EBITDA=7.96, Revenue=1.294, Gross Profit=2.886, EBITDA=7.662, Net Income Avl to Common =0.1, Diluted EPS=0.995, Earnings Growth=17.015, Revenue Growth=1.194, Total Cash=2.189, Total Cash Per Share=1.095, Total Debt=8.159, Current Ratio=6.567, Book Value Per Share=0.199, Cash Flow=11.542, Beta=2.886)</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Overview of the missing values in the key_stats dataframe\n\nOut[9]: Row(Date=0.0, Ticker=0.0, Price=1.99, DE Ratio=18.607, Trailing P/E=9.652, Price/Sales=0.498, Price/Book=2.189, Profit Margin=1.194, Operating Margin=0.498, Return on Assets=1.99, Return on Equity=2.786, Revenue Per Share=0.398, Market Cap=0.199, Enterprise Value=1.095, Forward P/E=46.269, PEG Ratio=4.776, Enterprise Value/Revenue=1.294, Enterprise Value/EBITDA=7.96, Revenue=1.294, Gross Profit=2.886, EBITDA=7.662, Net Income Avl to Common =0.1, Diluted EPS=0.995, Earnings Growth=17.015, Revenue Growth=1.194, Total Cash=2.189, Total Cash Per Share=1.095, Total Debt=8.159, Current Ratio=6.567, Book Value Per Share=0.199, Cash Flow=11.542, Beta=2.886)</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Missing values imputation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db3487d2-05a9-43f2-a9d0-1feb1ece31f9"}}},{"cell_type":"code","source":["summary = prices_df_nan_summary(prices_dfs, dfs_names)\npx.bar(summary, x='Stock name', y='Missing data (%)', hover_data=['Count'], title=\"Stock price dataset before preprocessing (only columns with missing values are displayed)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c77cdd70-d8af-43db-b097-f9ce91f1c266"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\rGenerating prices summary ...:   0%|          | 0/9 [00:00&lt;?, ?it/s]\rGenerating prices summary ...:  11%|█         | 1/9 [00:01&lt;00:09,  1.19s/it]\rGenerating prices summary ...:  22%|██▏       | 2/9 [00:02&lt;00:07,  1.01s/it]\rGenerating prices summary ...:  33%|███▎      | 3/9 [00:03&lt;00:07,  1.23s/it]\rGenerating prices summary ...:  44%|████▍     | 4/9 [00:05&lt;00:06,  1.39s/it]\rGenerating prices summary ...:  56%|█████▌    | 5/9 [00:05&lt;00:04,  1.11s/it]\rGenerating prices summary ...:  67%|██████▋   | 6/9 [00:06&lt;00:02,  1.03it/s]\rGenerating prices summary ...:  78%|███████▊  | 7/9 [00:07&lt;00:01,  1.13it/s]\rGenerating prices summary ...:  89%|████████▉ | 8/9 [00:07&lt;00:00,  1.20it/s]\rGenerating prices summary ...: 100%|██████████| 9/9 [00:08&lt;00:00,  1.25it/s]\rGenerating prices summary ...: 100%|██████████| 9/9 [00:08&lt;00:00,  1.04it/s]\nOut[10]: </div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\rGenerating prices summary ...:   0%|          | 0/9 [00:00&lt;?, ?it/s]\rGenerating prices summary ...:  11%|█         | 1/9 [00:01&lt;00:09,  1.19s/it]\rGenerating prices summary ...:  22%|██▏       | 2/9 [00:02&lt;00:07,  1.01s/it]\rGenerating prices summary ...:  33%|███▎      | 3/9 [00:03&lt;00:07,  1.23s/it]\rGenerating prices summary ...:  44%|████▍     | 4/9 [00:05&lt;00:06,  1.39s/it]\rGenerating prices summary ...:  56%|█████▌    | 5/9 [00:05&lt;00:04,  1.11s/it]\rGenerating prices summary ...:  67%|██████▋   | 6/9 [00:06&lt;00:02,  1.03it/s]\rGenerating prices summary ...:  78%|███████▊  | 7/9 [00:07&lt;00:01,  1.13it/s]\rGenerating prices summary ...:  89%|████████▉ | 8/9 [00:07&lt;00:00,  1.20it/s]\rGenerating prices summary ...: 100%|██████████| 9/9 [00:08&lt;00:00,  1.25it/s]\rGenerating prices summary ...: 100%|██████████| 9/9 [00:08&lt;00:00,  1.04it/s]\nOut[10]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>                <div id=\"906fe49e-8204-4453-863a-65683bc7b475\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"906fe49e-8204-4453-863a-65683bc7b475\")) {                    Plotly.newPlot(                        \"906fe49e-8204-4453-863a-65683bc7b475\",                        [{\"alignmentgroup\": \"True\", \"customdata\": [[1]], \"hovertemplate\": \"Stock name=%{x}<br>Missing data (%)=%{y}<br>Count=%{customdata[0]}<extra></extra>\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"AAPL\"], \"xaxis\": \"x\", \"y\": [0.01], \"yaxis\": \"y\"}],                        {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Stock price dataset before preprocessing (only columns with missing values are displayed)\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Stock name\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Missing data (%)\"}}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n</body>\n</html>","textData":null,"removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>                <div id=\"906fe49e-8204-4453-863a-65683bc7b475\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"906fe49e-8204-4453-863a-65683bc7b475\")) {                    Plotly.newPlot(                        \"906fe49e-8204-4453-863a-65683bc7b475\",                        [{\"alignmentgroup\": \"True\", \"customdata\": [[1]], \"hovertemplate\": \"Stock name=%{x}<br>Missing data (%)=%{y}<br>Count=%{customdata[0]}<extra></extra>\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"AAPL\"], \"xaxis\": \"x\", \"y\": [0.01], \"yaxis\": \"y\"}],                        {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Stock price dataset before preprocessing (only columns with missing values are displayed)\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Stock name\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Missing data (%)\"}}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n</body>\n</html>"]}}],"execution_count":0},{"cell_type":"markdown","source":["For most of the above stocks with missing values, we noticed that they indeed exist up to a given time and after that no more data is available. It may due to a business failure, hence no more stocks will be exchanged from that moment on."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46156c58-76a8-40c8-88a3-b4bf0a800dd5"}}},{"cell_type":"code","source":["# Clear our input data from training NaN values\nprices_dfs_new = [remove_trailing_nan(df,name) for df,name in tqdm(zip(prices_dfs, dfs_names), total=len(prices_dfs), desc='Removing trailing NaN values ...')]\n\n# Remove INTH stock from our dataset since it contains many inactivity periods\n#inth_idx = dfs_names.index('INTH') #TODO: uncomment\n#del dfs_names[inth_idx] #TODO: uncomment\n#del prices_dfs_new[inth_idx] #TODO: uncomment\n\nsummary = prices_df_nan_summary(prices_dfs_new, dfs_names)\npx.bar(summary, x='Stock name', y='Missing data (%)', hover_data=['Count'], title=\"Stock price dataset after preprocessing (only columns with missing values are displayed)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5884e89a-4386-4042-bd50-39dd70e3cef8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\rRemoving trailing NaN values ...:   0%|          | 0/9 [00:00&lt;?, ?it/s]\rRemoving trailing NaN values ...:  11%|█         | 1/9 [00:03&lt;00:26,  3.27s/it]\rRemoving trailing NaN values ...:  22%|██▏       | 2/9 [00:05&lt;00:17,  2.53s/it]\rRemoving trailing NaN values ...:  33%|███▎      | 3/9 [00:08&lt;00:15,  2.65s/it]\rRemoving trailing NaN values ...:  44%|████▍     | 4/9 [00:10&lt;00:12,  2.56s/it]\rRemoving trailing NaN values ...:  56%|█████▌    | 5/9 [00:12&lt;00:09,  2.32s/it]\rRemoving trailing NaN values ...:  67%|██████▋   | 6/9 [00:14&lt;00:06,  2.14s/it]\rRemoving trailing NaN values ...:  78%|███████▊  | 7/9 [00:15&lt;00:03,  1.94s/it]\rRemoving trailing NaN values ...:  89%|████████▉ | 8/9 [00:17&lt;00:01,  1.93s/it]\rRemoving trailing NaN values ...: 100%|██████████| 9/9 [00:19&lt;00:00,  1.79s/it]\rRemoving trailing NaN values ...: 100%|██████████| 9/9 [00:19&lt;00:00,  2.12s/it]\n\rGenerating prices summary ...:   0%|          | 0/9 [00:00&lt;?, ?it/s]\rGenerating prices summary ...:  11%|█         | 1/9 [00:02&lt;00:16,  2.08s/it]\rGenerating prices summary ...:  22%|██▏       | 2/9 [00:03&lt;00:11,  1.60s/it]\rGenerating prices summary ...:  33%|███▎      | 3/9 [00:04&lt;00:09,  1.51s/it]\rGenerating prices summary ...:  44%|████▍     | 4/9 [00:07&lt;00:09,  1.92s/it]\rGenerating prices summary ...:  56%|█████▌    | 5/9 [00:08&lt;00:06,  1.65s/it]\rGenerating prices summary ...:  67%|██████▋   | 6/9 [00:09&lt;00:04,  1.52s/it]\rGenerating prices summary ...:  78%|███████▊  | 7/9 [00:10&lt;00:02,  1.43s/it]\rGenerating prices summary ...:  89%|████████▉ | 8/9 [00:12&lt;00:01,  1.51s/it]\rGenerating prices summary ...: 100%|██████████| 9/9 [00:14&lt;00:00,  1.49s/it]\rGenerating prices summary ...: 100%|██████████| 9/9 [00:14&lt;00:00,  1.57s/it]\nOut[11]: </div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\rRemoving trailing NaN values ...:   0%|          | 0/9 [00:00&lt;?, ?it/s]\rRemoving trailing NaN values ...:  11%|█         | 1/9 [00:03&lt;00:26,  3.27s/it]\rRemoving trailing NaN values ...:  22%|██▏       | 2/9 [00:05&lt;00:17,  2.53s/it]\rRemoving trailing NaN values ...:  33%|███▎      | 3/9 [00:08&lt;00:15,  2.65s/it]\rRemoving trailing NaN values ...:  44%|████▍     | 4/9 [00:10&lt;00:12,  2.56s/it]\rRemoving trailing NaN values ...:  56%|█████▌    | 5/9 [00:12&lt;00:09,  2.32s/it]\rRemoving trailing NaN values ...:  67%|██████▋   | 6/9 [00:14&lt;00:06,  2.14s/it]\rRemoving trailing NaN values ...:  78%|███████▊  | 7/9 [00:15&lt;00:03,  1.94s/it]\rRemoving trailing NaN values ...:  89%|████████▉ | 8/9 [00:17&lt;00:01,  1.93s/it]\rRemoving trailing NaN values ...: 100%|██████████| 9/9 [00:19&lt;00:00,  1.79s/it]\rRemoving trailing NaN values ...: 100%|██████████| 9/9 [00:19&lt;00:00,  2.12s/it]\n\rGenerating prices summary ...:   0%|          | 0/9 [00:00&lt;?, ?it/s]\rGenerating prices summary ...:  11%|█         | 1/9 [00:02&lt;00:16,  2.08s/it]\rGenerating prices summary ...:  22%|██▏       | 2/9 [00:03&lt;00:11,  1.60s/it]\rGenerating prices summary ...:  33%|███▎      | 3/9 [00:04&lt;00:09,  1.51s/it]\rGenerating prices summary ...:  44%|████▍     | 4/9 [00:07&lt;00:09,  1.92s/it]\rGenerating prices summary ...:  56%|█████▌    | 5/9 [00:08&lt;00:06,  1.65s/it]\rGenerating prices summary ...:  67%|██████▋   | 6/9 [00:09&lt;00:04,  1.52s/it]\rGenerating prices summary ...:  78%|███████▊  | 7/9 [00:10&lt;00:02,  1.43s/it]\rGenerating prices summary ...:  89%|████████▉ | 8/9 [00:12&lt;00:01,  1.51s/it]\rGenerating prices summary ...: 100%|██████████| 9/9 [00:14&lt;00:00,  1.49s/it]\rGenerating prices summary ...: 100%|██████████| 9/9 [00:14&lt;00:00,  1.57s/it]\nOut[11]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>                <div id=\"9047134e-d082-4a10-9022-09ef1aaadee5\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9047134e-d082-4a10-9022-09ef1aaadee5\")) {                    Plotly.newPlot(                        \"9047134e-d082-4a10-9022-09ef1aaadee5\",                        [{\"alignmentgroup\": \"True\", \"customdata\": [[1]], \"hovertemplate\": \"Stock name=%{x}<br>Missing data (%)=%{y}<br>Count=%{customdata[0]}<extra></extra>\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"AAPL\"], \"xaxis\": \"x\", \"y\": [0.01], \"yaxis\": \"y\"}],                        {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Stock price dataset after preprocessing (only columns with missing values are displayed)\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Stock name\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Missing data (%)\"}}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n</body>\n</html>","textData":null,"removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>                <div id=\"9047134e-d082-4a10-9022-09ef1aaadee5\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9047134e-d082-4a10-9022-09ef1aaadee5\")) {                    Plotly.newPlot(                        \"9047134e-d082-4a10-9022-09ef1aaadee5\",                        [{\"alignmentgroup\": \"True\", \"customdata\": [[1]], \"hovertemplate\": \"Stock name=%{x}<br>Missing data (%)=%{y}<br>Count=%{customdata[0]}<extra></extra>\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"AAPL\"], \"xaxis\": \"x\", \"y\": [0.01], \"yaxis\": \"y\"}],                        {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Stock price dataset after preprocessing (only columns with missing values are displayed)\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Stock name\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Missing data (%)\"}}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n</body>\n</html>"]}}],"execution_count":0},{"cell_type":"markdown","source":["At this point we use the fast forward imputation technique to fill-in missing values. Please note that in this case missing values are mostly due to holidays or periods when stocks are not exchanged."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b219a552-fd13-453f-826c-71fb94bfd81f"}}},{"cell_type":"markdown","source":["### Building our new dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3595c39e-c759-41d4-a927-ab75e577b16e"}}},{"cell_type":"code","source":["# Impute missing values in the prices dataset (i.e. fast-forward last valid values)\n\n# define the window\nwindow = Window.orderBy('Date')\\\n               .rowsBetween(Window.unboundedPreceding, 0)\n\n# Forward filling values \n# (ref. https://stackoverflow.com/questions/38131982/forward-fill-missing-values-in-spark-python/50422240#50422240)\nfor i in range(len(prices_dfs_new)):\n  for col_name in prices_dfs_new[i].schema.names:\n    col = F.last(prices_dfs_new[i][col_name], ignorenulls=True).over(window)\n    prices_dfs_new[i] = prices_dfs_new[i].withColumn(col_name, col)\n\n# In this case this dataframe contains financial reports that may contain NaN values either because that\n# metric was not available at that time OR because it was monitoring an initial stage of a company growth.\n# What we do is to apply the classic fast-forward, and fill initial missing values with zeroes.\n# Please note: we also discard the 'Forward P/E' column since the imputation here would introduce too much noise.\nkey_stats_df_new = key_stats_df.drop('Forward P/E')\nfor col_name in key_stats_df_new.schema.names:\n    col = F.last(key_stats_df_new[col_name], ignorenulls=True).over(window)\n    key_stats_df_new = key_stats_df_new.withColumn(col_name, col)\nkey_stats_df_new = key_stats_df_new.fillna(0.)\n\n\n# Before merging the two datasets we need to sort them\n#prices_dfs_new = [df.orderBy('Date').reset_index(drop=True) for df in prices_dfs_new]\n#key_stats_df_new = key_stats_df_new.sort_values(by=['Date']).reset_index(drop=True)\n\n\n# Merge the stock price dataset with fundamental data of the relative company\n#aggregate_dfs = merge_prices_fundamentals(prices_dfs_new, key_stats_df_new, dfs_names)\n                  \n# TODO: add more functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b215510b-4ef8-4b04-b097-c467153c4c84"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\rMerging the datasets ...:   0%|          | 0/56 [00:00&lt;?, ?it/s]\rMerging the datasets ...:   2%|▏         | 1/56 [00:01&lt;01:05,  1.18s/it]\rMerging the datasets ...:   4%|▎         | 2/56 [00:02&lt;01:18,  1.46s/it]\rMerging the datasets ...:   5%|▌         | 3/56 [00:04&lt;01:11,  1.36s/it]\rMerging the datasets ...:   7%|▋         | 4/56 [00:05&lt;01:07,  1.29s/it]\rMerging the datasets ...:   9%|▉         | 5/56 [00:06&lt;01:07,  1.32s/it]\rMerging the datasets ...:  11%|█         | 6/56 [00:07&lt;01:01,  1.23s/it]\rMerging the datasets ...:  12%|█▎        | 7/56 [00:09&lt;01:08,  1.39s/it]\rMerging the datasets ...:  14%|█▍        | 8/56 [00:11&lt;01:15,  1.57s/it]\rMerging the datasets ...:  16%|█▌        | 9/56 [00:12&lt;01:14,  1.59s/it]\rMerging the datasets ...:  16%|█▌        | 9/56 [00:14&lt;01:15,  1.61s/it]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\rMerging the datasets ...:   0%|          | 0/56 [00:00&lt;?, ?it/s]\rMerging the datasets ...:   2%|▏         | 1/56 [00:01&lt;01:05,  1.18s/it]\rMerging the datasets ...:   4%|▎         | 2/56 [00:02&lt;01:18,  1.46s/it]\rMerging the datasets ...:   5%|▌         | 3/56 [00:04&lt;01:11,  1.36s/it]\rMerging the datasets ...:   7%|▋         | 4/56 [00:05&lt;01:07,  1.29s/it]\rMerging the datasets ...:   9%|▉         | 5/56 [00:06&lt;01:07,  1.32s/it]\rMerging the datasets ...:  11%|█         | 6/56 [00:07&lt;01:01,  1.23s/it]\rMerging the datasets ...:  12%|█▎        | 7/56 [00:09&lt;01:08,  1.39s/it]\rMerging the datasets ...:  14%|█▍        | 8/56 [00:11&lt;01:15,  1.57s/it]\rMerging the datasets ...:  16%|█▌        | 9/56 [00:12&lt;01:14,  1.59s/it]\rMerging the datasets ...:  16%|█▌        | 9/56 [00:14&lt;01:15,  1.61s/it]\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3715022015688392&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     63</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span> <span class=\"ansi-red-fg\"># Merge the stock price dataset with fundamental data of the relative company</span>\n<span class=\"ansi-green-fg\">---&gt; 65</span><span class=\"ansi-red-fg\"> </span>aggregate_dfs <span class=\"ansi-blue-fg\">=</span> merge_prices_fundamentals<span class=\"ansi-blue-fg\">(</span>prices_dfs_new<span class=\"ansi-blue-fg\">,</span> key_stats_df_new<span class=\"ansi-blue-fg\">,</span> dfs_names<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     66</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span> <span class=\"ansi-red-fg\"># TODO: add more functions</span>\n\n<span class=\"ansi-green-fg\">&lt;command-3715022015688392&gt;</span> in <span class=\"ansi-cyan-fg\">merge_prices_fundamentals</span><span class=\"ansi-blue-fg\">(prices_dfs, key_stats_df, dfs_names, drop_cols)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     59</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     60</span>     <span class=\"ansi-red-fg\"># The current financial report</span>\n<span class=\"ansi-green-fg\">---&gt; 61</span><span class=\"ansi-red-fg\">     </span>ticker_iterator <span class=\"ansi-blue-fg\">=</span> list<span class=\"ansi-blue-fg\">(</span>ticker_df<span class=\"ansi-blue-fg\">.</span>iterrows<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>     ticker_iter_idx <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-cyan-fg\">0</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     63</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">__getattr__</span><span class=\"ansi-blue-fg\">(self, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1664</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   1665</span>         <span class=\"ansi-green-fg\">if</span> name <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1666</span><span class=\"ansi-red-fg\">             raise AttributeError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1667</span>                 &#34;&#39;%s&#39; object has no attribute &#39;%s&#39;&#34; % (self.__class__.__name__, name))\n<span class=\"ansi-green-intense-fg ansi-bold\">   1668</span>         jc <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>apply<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;iterrows&#39;</div>","errorSummary":"<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;iterrows&#39;","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3715022015688392&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     63</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span> <span class=\"ansi-red-fg\"># Merge the stock price dataset with fundamental data of the relative company</span>\n<span class=\"ansi-green-fg\">---&gt; 65</span><span class=\"ansi-red-fg\"> </span>aggregate_dfs <span class=\"ansi-blue-fg\">=</span> merge_prices_fundamentals<span class=\"ansi-blue-fg\">(</span>prices_dfs_new<span class=\"ansi-blue-fg\">,</span> key_stats_df_new<span class=\"ansi-blue-fg\">,</span> dfs_names<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     66</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span> <span class=\"ansi-red-fg\"># TODO: add more functions</span>\n\n<span class=\"ansi-green-fg\">&lt;command-3715022015688392&gt;</span> in <span class=\"ansi-cyan-fg\">merge_prices_fundamentals</span><span class=\"ansi-blue-fg\">(prices_dfs, key_stats_df, dfs_names, drop_cols)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     59</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     60</span>     <span class=\"ansi-red-fg\"># The current financial report</span>\n<span class=\"ansi-green-fg\">---&gt; 61</span><span class=\"ansi-red-fg\">     </span>ticker_iterator <span class=\"ansi-blue-fg\">=</span> list<span class=\"ansi-blue-fg\">(</span>ticker_df<span class=\"ansi-blue-fg\">.</span>iterrows<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>     ticker_iter_idx <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-cyan-fg\">0</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     63</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">__getattr__</span><span class=\"ansi-blue-fg\">(self, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1664</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   1665</span>         <span class=\"ansi-green-fg\">if</span> name <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1666</span><span class=\"ansi-red-fg\">             raise AttributeError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1667</span>                 &#34;&#39;%s&#39; object has no attribute &#39;%s&#39;&#34; % (self.__class__.__name__, name))\n<span class=\"ansi-green-intense-fg ansi-bold\">   1668</span>         jc <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>apply<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;DataFrame&#39; object has no attribute &#39;iterrows&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["key_stats_df_new"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af9fd69f-96fe-4a8e-b5de-c8dc57b6443c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[39]: DataFrame[Date: date, Ticker: string, Price: double, DE Ratio: double, Trailing P/E: double, Price/Sales: double, Price/Book: double, Profit Margin: double, Operating Margin: double, Return on Assets: double, Return on Equity: double, Revenue Per Share: double, Market Cap: double, Enterprise Value: double, PEG Ratio: double, Enterprise Value/Revenue: double, Enterprise Value/EBITDA: double, Revenue: double, Gross Profit: double, EBITDA: double, Net Income Avl to Common : double, Diluted EPS: double, Earnings Growth: double, Revenue Growth: double, Total Cash: double, Total Cash Per Share: double, Total Debt: double, Current Ratio: double, Book Value Per Share: double, Cash Flow: double, Beta: double]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[39]: DataFrame[Date: date, Ticker: string, Price: double, DE Ratio: double, Trailing P/E: double, Price/Sales: double, Price/Book: double, Profit Margin: double, Operating Margin: double, Return on Assets: double, Return on Equity: double, Revenue Per Share: double, Market Cap: double, Enterprise Value: double, PEG Ratio: double, Enterprise Value/Revenue: double, Enterprise Value/EBITDA: double, Revenue: double, Gross Profit: double, EBITDA: double, Net Income Avl to Common : double, Diluted EPS: double, Earnings Growth: double, Revenue Growth: double, Total Cash: double, Total Cash Per Share: double, Total Debt: double, Current Ratio: double, Book Value Per Share: double, Cash Flow: double, Beta: double]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Technical indicators"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f24fd395-0f93-4b3b-8893-40e0d3105789"}}},{"cell_type":"code","source":["def add_col_sma(pair):\n          return dict(pair[0].asDict().items() + [(\"SMA\", pair[1])])\n\ndef add_sma(dfs: List[pyspark.sql.DataFrame], period: int = 10) -> None:\n    ''' Computes the Simple Moving Average from a given dataframe. '''\n    for i in tqdm(range(len(dfs)), desc='Adding SMA ...'):\n        ret = np.cumsum(dfs[i][\"Adjusted Close\"])#.to_numpy()\n        ret[period:] = ret[period:] - ret[:-period]\n        sma = ret[period - 1:] / period\n        missing_values = dfs[i].count() - sma.shape[0]\n        half_missing_values = missing_values//2\n        sma_points = np.empty(dfs[i].count(), dtype=float)\n        sma_points[half_missing_values:(half_missing_values)+sma.shape[0]] = sma\n\n        first_values = np.array([row[\"Adjusted Close\"] for row in dfs[i].head(half_missing_values)])\n        last_values = np.array([row[\"Adjusted Close\"] for row in dfs[i].orderBy(F.desc(\"Date\")).take(half_missing_values+sma.shape[0])])[::-1]\n\n        sma_points[:half_missing_values] = first_values\n        sma_points[half_missing_values+sma.shape[0]:] = last_values\n        \n        n = dfs[i].rdd.getNumPartitions()\n        \n        sma_col = sc.parallelize(sma_points, n).map(float)\n\n        rdd = (dfs[i]\n            .rdd # Extract RDD\n            .zip(sma_col) # Zip with new col\n            .map(add_col_sma)) # Add new column\n\n        \n          \n        dfs[i] = sqlContext.createDataFrame(rdd)\n        \n        "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d73d175-ad28-40ec-bc79-f4fff8d5242c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dfs = []\n\ndf1 = spark.createDataFrame(\n    [\n        (1, 0.5), \n        (2, 0.6),\n        (3, 0.7),\n        (4, 0.9),\n        (5, 1.2),\n        (6, 0.8),\n        (7, 0.4),\n        (8, 1.6),\n        (9, 1.7),\n        (10, 0.5), \n        (11, 0.6),\n        (12, 0.7),\n        (13, 0.9),\n        (14, 1.2),\n        (15, 0.8),\n        (16, 0.4),\n        (17, 1.6),\n        (18, 1.7),\n    ],\n    [\"Date\", \"Adjusted Close\"]  # add your column names here\n)\n\ndfs = [df1]\n\n# Add SMA indicator to each dataframe\nadd_sma(dfs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2dd07781-8dc4-43d6-a81b-7c2361ec5eb2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\rAdding SMA ...:   0%|          | 0/1 [00:00&lt;?, ?it/s]PythonRDD[1676] at RDD at PythonRDD.scala:58\n\rAdding SMA ...:   0%|          | 0/1 [00:01&lt;?, ?it/s]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\rAdding SMA ...:   0%|          | 0/1 [00:00&lt;?, ?it/s]PythonRDD[1676] at RDD at PythonRDD.scala:58\n\rAdding SMA ...:   0%|          | 0/1 [00:01&lt;?, ?it/s]\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2993182863203180&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span> <span class=\"ansi-red-fg\"># Add SMA indicator to each dataframe</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\"> </span>add_sma<span class=\"ansi-blue-fg\">(</span>dfs<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-2993182863203179&gt;</span> in <span class=\"ansi-cyan-fg\">add_sma</span><span class=\"ansi-blue-fg\">(dfs, period)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>         print<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     32</span> \n<span class=\"ansi-green-fg\">---&gt; 33</span><span class=\"ansi-red-fg\">         </span>dfs<span class=\"ansi-blue-fg\">[</span>i<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> sqlContext<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     34</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     35</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/context.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    384</span>         Py4JJavaError<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">...</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    385</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 386</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>sparkSession<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>     <span class=\"ansi-green-fg\">def</span> registerDataFrameAsTable<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> df<span class=\"ansi-blue-fg\">,</span> tableName<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    720</span>             return super(SparkSession, self).createDataFrame(\n<span class=\"ansi-green-intense-fg ansi-bold\">    721</span>                 data, schema, samplingRatio, verifySchema)\n<span class=\"ansi-green-fg\">--&gt; 722</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_create_dataframe<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    723</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    724</span>     <span class=\"ansi-green-fg\">def</span> _create_dataframe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_create_dataframe</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    750</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    751</span>             <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> RDD<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 752</span><span class=\"ansi-red-fg\">                 </span>rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromRDD<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    753</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    754</span>                 rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromLocal<span class=\"ansi-blue-fg\">(</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_createFromRDD</span><span class=\"ansi-blue-fg\">(self, rdd, schema, samplingRatio)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    485</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    486</span>         <span class=\"ansi-green-fg\">if</span> schema <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">or</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>list<span class=\"ansi-blue-fg\">,</span> tuple<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 487</span><span class=\"ansi-red-fg\">             </span>struct <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_inferSchema<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> names<span class=\"ansi-blue-fg\">=</span>schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    488</span>             converter <span class=\"ansi-blue-fg\">=</span> _create_converter<span class=\"ansi-blue-fg\">(</span>struct<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    489</span>             rdd <span class=\"ansi-blue-fg\">=</span> rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>converter<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_inferSchema</span><span class=\"ansi-blue-fg\">(self, rdd, samplingRatio, names)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    459</span>         <span class=\"ansi-blue-fg\">:</span><span class=\"ansi-green-fg\">class</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-red-fg\">`</span>pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>types<span class=\"ansi-blue-fg\">.</span>StructType<span class=\"ansi-red-fg\">`</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    460</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 461</span><span class=\"ansi-red-fg\">         </span>first <span class=\"ansi-blue-fg\">=</span> rdd<span class=\"ansi-blue-fg\">.</span>first<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    462</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-green-fg\">not</span> first<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    463</span>             raise ValueError(&#34;The first row in RDD is empty, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">first</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1619</span>         ValueError<span class=\"ansi-blue-fg\">:</span> RDD <span class=\"ansi-green-fg\">is</span> empty\n<span class=\"ansi-green-intense-fg ansi-bold\">   1620</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1621</span><span class=\"ansi-red-fg\">         </span>rs <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1622</span>         <span class=\"ansi-green-fg\">if</span> rs<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1623</span>             <span class=\"ansi-green-fg\">return</span> rs<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1599</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1600</span>             p <span class=\"ansi-blue-fg\">=</span> range<span class=\"ansi-blue-fg\">(</span>partsScanned<span class=\"ansi-blue-fg\">,</span> min<span class=\"ansi-blue-fg\">(</span>partsScanned <span class=\"ansi-blue-fg\">+</span> numPartsToTry<span class=\"ansi-blue-fg\">,</span> totalParts<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1601</span><span class=\"ansi-red-fg\">             </span>res <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> takeUpToNumLeft<span class=\"ansi-blue-fg\">,</span> p<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1602</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1603</span>             items <span class=\"ansi-blue-fg\">+=</span> res\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">runJob</span><span class=\"ansi-blue-fg\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1339</span>             <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1340</span>                 os<span class=\"ansi-blue-fg\">.</span>remove<span class=\"ansi-blue-fg\">(</span>filename<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1341</span><span class=\"ansi-red-fg\">         </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsc<span class=\"ansi-blue-fg\">.</span>sc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">,</span> partitions<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1342</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1343</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 847.0 failed 1 times, most recent failure: Lost task 0.0 in stage 847.0 (TID 1994) (ip-10-172-247-22.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;TypeError: unsupported operand type(s) for +: &#39;dict_items&#39; and &#39;list&#39;&#39;, from &lt;command-2993182863203179&gt;, line 2. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 701, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 693, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1595, in takeUpToNumLeft\n    yield next(iterator)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 73, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-2993182863203179&gt;&#34;, line 2, in add_col_sma\nTypeError: unsupported operand type(s) for +: &#39;dict_items&#39; and &#39;list&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$collectPartitions$1(PythonRDD.scala:197)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2492)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:119)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2711)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2658)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1255)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2848)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2452)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2435)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2492)\n\tat org.apache.spark.api.python.PythonRDD$.collectPartitions(PythonRDD.scala:197)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:217)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor572.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;TypeError: unsupported operand type(s) for +: &#39;dict_items&#39; and &#39;list&#39;&#39;, from &lt;command-2993182863203179&gt;, line 2. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 701, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 693, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1595, in takeUpToNumLeft\n    yield next(iterator)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 73, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-2993182863203179&gt;&#34;, line 2, in add_col_sma\nTypeError: unsupported operand type(s) for +: &#39;dict_items&#39; and &#39;list&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$collectPartitions$1(PythonRDD.scala:197)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2492)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:119)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 847.0 failed 1 times, most recent failure: Lost task 0.0 in stage 847.0 (TID 1994) (ip-10-172-247-22.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;TypeError: unsupported operand type(s) for +: &#39;dict_items&#39; and &#39;list&#39;&#39;, from &lt;command-2993182863203179&gt;, line 2. Full traceback below:","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2993182863203180&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span> <span class=\"ansi-red-fg\"># Add SMA indicator to each dataframe</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\"> </span>add_sma<span class=\"ansi-blue-fg\">(</span>dfs<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-2993182863203179&gt;</span> in <span class=\"ansi-cyan-fg\">add_sma</span><span class=\"ansi-blue-fg\">(dfs, period)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>         print<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     32</span> \n<span class=\"ansi-green-fg\">---&gt; 33</span><span class=\"ansi-red-fg\">         </span>dfs<span class=\"ansi-blue-fg\">[</span>i<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> sqlContext<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     34</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     35</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/context.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    384</span>         Py4JJavaError<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">...</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    385</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 386</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>sparkSession<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>     <span class=\"ansi-green-fg\">def</span> registerDataFrameAsTable<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> df<span class=\"ansi-blue-fg\">,</span> tableName<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">createDataFrame</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    720</span>             return super(SparkSession, self).createDataFrame(\n<span class=\"ansi-green-intense-fg ansi-bold\">    721</span>                 data, schema, samplingRatio, verifySchema)\n<span class=\"ansi-green-fg\">--&gt; 722</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_create_dataframe<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    723</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    724</span>     <span class=\"ansi-green-fg\">def</span> _create_dataframe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> verifySchema<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_create_dataframe</span><span class=\"ansi-blue-fg\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    750</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    751</span>             <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">,</span> RDD<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 752</span><span class=\"ansi-red-fg\">                 </span>rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromRDD<span class=\"ansi-blue-fg\">(</span>data<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    753</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    754</span>                 rdd<span class=\"ansi-blue-fg\">,</span> schema <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_createFromLocal<span class=\"ansi-blue-fg\">(</span>map<span class=\"ansi-blue-fg\">(</span>prepare<span class=\"ansi-blue-fg\">,</span> data<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> schema<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_createFromRDD</span><span class=\"ansi-blue-fg\">(self, rdd, schema, samplingRatio)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    485</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    486</span>         <span class=\"ansi-green-fg\">if</span> schema <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">or</span> isinstance<span class=\"ansi-blue-fg\">(</span>schema<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>list<span class=\"ansi-blue-fg\">,</span> tuple<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 487</span><span class=\"ansi-red-fg\">             </span>struct <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_inferSchema<span class=\"ansi-blue-fg\">(</span>rdd<span class=\"ansi-blue-fg\">,</span> samplingRatio<span class=\"ansi-blue-fg\">,</span> names<span class=\"ansi-blue-fg\">=</span>schema<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    488</span>             converter <span class=\"ansi-blue-fg\">=</span> _create_converter<span class=\"ansi-blue-fg\">(</span>struct<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    489</span>             rdd <span class=\"ansi-blue-fg\">=</span> rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>converter<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansi-cyan-fg\">_inferSchema</span><span class=\"ansi-blue-fg\">(self, rdd, samplingRatio, names)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    459</span>         <span class=\"ansi-blue-fg\">:</span><span class=\"ansi-green-fg\">class</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-red-fg\">`</span>pyspark<span class=\"ansi-blue-fg\">.</span>sql<span class=\"ansi-blue-fg\">.</span>types<span class=\"ansi-blue-fg\">.</span>StructType<span class=\"ansi-red-fg\">`</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    460</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 461</span><span class=\"ansi-red-fg\">         </span>first <span class=\"ansi-blue-fg\">=</span> rdd<span class=\"ansi-blue-fg\">.</span>first<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    462</span>         <span class=\"ansi-green-fg\">if</span> <span class=\"ansi-green-fg\">not</span> first<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    463</span>             raise ValueError(&#34;The first row in RDD is empty, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">first</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1619</span>         ValueError<span class=\"ansi-blue-fg\">:</span> RDD <span class=\"ansi-green-fg\">is</span> empty\n<span class=\"ansi-green-intense-fg ansi-bold\">   1620</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1621</span><span class=\"ansi-red-fg\">         </span>rs <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>take<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1622</span>         <span class=\"ansi-green-fg\">if</span> rs<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1623</span>             <span class=\"ansi-green-fg\">return</span> rs<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">take</span><span class=\"ansi-blue-fg\">(self, num)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1599</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1600</span>             p <span class=\"ansi-blue-fg\">=</span> range<span class=\"ansi-blue-fg\">(</span>partsScanned<span class=\"ansi-blue-fg\">,</span> min<span class=\"ansi-blue-fg\">(</span>partsScanned <span class=\"ansi-blue-fg\">+</span> numPartsToTry<span class=\"ansi-blue-fg\">,</span> totalParts<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1601</span><span class=\"ansi-red-fg\">             </span>res <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> takeUpToNumLeft<span class=\"ansi-blue-fg\">,</span> p<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1602</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1603</span>             items <span class=\"ansi-blue-fg\">+=</span> res\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansi-cyan-fg\">runJob</span><span class=\"ansi-blue-fg\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1339</span>             <span class=\"ansi-green-fg\">finally</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1340</span>                 os<span class=\"ansi-blue-fg\">.</span>remove<span class=\"ansi-blue-fg\">(</span>filename<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1341</span><span class=\"ansi-red-fg\">         </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>runJob<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jsc<span class=\"ansi-blue-fg\">.</span>sc<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">,</span> partitions<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1342</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> mappedRDD<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1343</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 847.0 failed 1 times, most recent failure: Lost task 0.0 in stage 847.0 (TID 1994) (ip-10-172-247-22.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: &#39;TypeError: unsupported operand type(s) for +: &#39;dict_items&#39; and &#39;list&#39;&#39;, from &lt;command-2993182863203179&gt;, line 2. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 701, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 693, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1595, in takeUpToNumLeft\n    yield next(iterator)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 73, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-2993182863203179&gt;&#34;, line 2, in add_col_sma\nTypeError: unsupported operand type(s) for +: &#39;dict_items&#39; and &#39;list&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$collectPartitions$1(PythonRDD.scala:197)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2492)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:119)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2711)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2658)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1255)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2848)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2452)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2435)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2473)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2492)\n\tat org.apache.spark.api.python.PythonRDD$.collectPartitions(PythonRDD.scala:197)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:217)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor572.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;TypeError: unsupported operand type(s) for +: &#39;dict_items&#39; and &#39;list&#39;&#39;, from &lt;command-2993182863203179&gt;, line 2. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 701, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 693, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 267, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/rdd.py&#34;, line 1595, in takeUpToNumLeft\n    yield next(iterator)\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 73, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-2993182863203179&gt;&#34;, line 2, in add_col_sma\nTypeError: unsupported operand type(s) for +: &#39;dict_items&#39; and &#39;list&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:661)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:813)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:795)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:614)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$collectPartitions$1(PythonRDD.scala:197)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$2(SparkContext.scala:2492)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:119)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"dist_forecasting","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3715022015688369}},"nbformat":4,"nbformat_minor":0}
