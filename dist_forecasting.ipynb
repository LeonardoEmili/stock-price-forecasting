{"cells":[{"cell_type":"markdown","source":["## Import required packages"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e862fc71-84e1-4af7-badc-898efadd0bc5"}}},{"cell_type":"code","source":["# Download required packages\n!pip -q install gdown missingno torch petastorm wandb\n\n%matplotlib inline\n\nimport pyspark\nfrom pyspark.sql import *\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql.window import Window\n\nfrom petastorm.spark import SparkDatasetConverter, make_spark_converter\nfrom petastorm.pytorch import DataLoader\nspark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, 'file:///dbfs/tmp/petastorm/cache')\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.sql.functions import udf\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport missingno as msno\nimport torch\nimport re\nimport torch.nn as nn\nfrom typing import *\nimport datetime\nimport gdown\nimport dataclasses\nfrom operator import itemgetter\nfrom functools import partial\n\nimport wandb\nwandb.login(key='147339090e59f6a02bed0fa3f938a2b1ecbc567c')\n\nimport tqdm as tq\ndef tqdm(*args, **kwargs):\n  ''' Small trick to prevent tqdm printing newlines at each step. '''\n  return tq.tqdm(*args, **kwargs, leave=True, position=0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d89c4672-97c1-49d3-9350-6008885da2ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"><span class=\"ansi-yellow-fg\">WARNING: You are using pip version 20.2.4; however, version 21.1.1 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: W&amp;B API key is configured (use `wandb login --relogin` to force relogin)\nwandb: WARNING If you&#39;re specifying your api key in code, ensure this code is not shared publically.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-yellow-fg\">WARNING: You are using pip version 20.2.4; however, version 21.1.1 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: W&amp;B API key is configured (use `wandb login --relogin` to force relogin)\nwandb: WARNING If you&#39;re specifying your api key in code, ensure this code is not shared publically.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Data aquisition\nWe retrieve our datasets and download them to a temporary directory in the driver node."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f76226d3-37ac-40d6-a417-a05ca05a7000"}}},{"cell_type":"code","source":["!rm -rf /tmp/data /tmp/__MACOSX\ngdown.download('https://drive.google.com/uc?id=1ggmDp-AWFzbQReLG0pLpQE_3fO0C0RnM', '/tmp/data.zip', quiet=False)\n!unzip -q /tmp/data.zip -d /tmp/\n!rm /tmp/data.zip"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c1efdeb-4084-4115-9920-536feac86d20"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Then we load the datasets to the DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"941ae011-778f-4659-9774-ff7df196b5b1"}}},{"cell_type":"code","source":["dbutils.fs.mv(\"file:/tmp/data\", \"dbfs:/data\", recurse=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"791fea85-d7b5-4432-b530-ddae393cc5d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs ls /data/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c18efa6b-4546-4cab-9e4b-582119acee64"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%fs ls /data/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5e7b9cc-713e-4303-a194-15a532c0757b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Dataset loading"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"898c47b5-01c8-4a5c-86c1-58bd6334f04a"}}},{"cell_type":"code","source":["key_stats_df = spark.read.load(\"dbfs:/data/key_stats_yahoo.csv\", \n                           format=\"csv\",\n                           sep=\",\",\n                           inferSchema=\"true\",\n                           header=\"true\"\n                          )\n\n# Drop the first ID column\nkey_stats_df = sc.parallelize(key_stats_df.drop(key_stats_df.columns[0]).head(1005)).toDF()#TODO: remove head(n) (only meant for development)\n#key_stats_df = sc.parallelize(key_stats_df.drop(key_stats_df.columns[0]).head(2000)).toDF()#TODO: remove head(n) (only meant for development)\n#key_stats_df = key_stats_df.drop(key_stats_df.columns[0])\nkey_stats_df.schema['Date'].nullable = False\n\n# Use legacy format to parse dates\nspark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\nkey_stats_df = key_stats_df.withColumn(\"Date\", F.to_date(key_stats_df[\"Date\"], 'MM/dd/yyyy HH:mm'))\n\n# Cast numerical columns to double\nfor column in key_stats_df.columns[2:]:\n  key_stats_df = key_stats_df.withColumn(column, key_stats_df[column].cast(\"double\"))\n\n# Prices dataframes for every stock #TODO: remove :N (only meant for development)\nprices_files = [f.path for f in dbutils.fs.ls('/data/prices/')[:10] if f.path.endswith('.csv')]\n#prices_files = [f.path for f in dbutils.fs.ls('/data/prices/')[:20] if f.path.endswith('.csv')]\n#prices_files = [f.path for f in dbutils.fs.ls('/data/prices/') if f.path.endswith('.csv')]\ndfs_names = [f.rsplit('/', 1)[1][:-len('.csv')] for f in prices_files]\nprices_dfs = []\nfor f in tqdm(prices_files, desc='Reading stock price data', total=len(prices_files)):\n  df = spark.read.load(f,\n                       format=\"csv\",\n                       sep=\",\",\n                       inferSchema=\"true\",\n                       header=\"true\"\n                      )\n  df = df.withColumn(\"Date\", F.to_date(df[\"Date\"], 'dd-MM-yyyy'))\n  df.schema['Date'].nullable = False\n  prices_dfs.append(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"832ad8e5-1aed-43f4-8748-ec7a4ced09bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Dataset analysis"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be1bb73d-1713-4401-9ca9-cef3f47a976f"}}},{"cell_type":"code","source":["print(\"Prices dataframe format:\")\nprices_dfs[0].printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd6040d0-f97c-4ef4-bd0d-3a1a8d4214a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"Key stats dataframe format:\")\nkey_stats_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc56a262-df2a-4a39-ab77-76f55ee2948c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Utility functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"578db780-5427-4c43-8252-682b1a10c6d0"}}},{"cell_type":"code","source":["# TODO: add remaining utility functions\n\ndef prices_df_nan_summary(prices_dfs: List[pyspark.sql.DataFrame], names: List[str]) -> pd.DataFrame:\n  ''' Utility function to summarize columns that have missing values. '''\n  nan_dfs = []\n  for prices_df, name in tqdm(zip(prices_dfs, names), total=len(prices_dfs), desc='Generating prices summary ...'):\n    nan_absolute = prices_df.agg(*[F.count(F.when(F.isnull(c), c)).alias(c) for c in prices_df.columns]).first()\n    if any(nan_absolute):\n      # Simple conversion from Pyspark row -> Python set of values\n      values = set(nan_absolute.asDict().values()).difference({0})\n      # Either we don't have values for that row, or we have all of them (but Date which is non-nullable)\n      # Values contains the no. of NaN values and 0 in correspondance of the Date column\n      assert len(values) == 1\n      nan_count = values.pop()\n      nan_dfs.append((name, round(100*nan_count/prices_df.count(), 3), nan_count))\n\n  return pd.DataFrame(nan_dfs, columns=['Stock name', 'Missing data (%)', 'Count'])\n\ndef remove_trailing_nan(df: pyspark.sql.DataFrame, ticker: str, col: str = 'Low') -> pyspark.sql.DataFrame:\n  '''\n    A trick to detect if the input DataFrame ends with a contiguous collection of NaN rows, returns the dataframe without them.\n  '''\n  # The total number of rows of the dataframe\n  df_length = df.count()\n  \n  # Sort the input dataframe and add a new column to keep track of the relative position of each row\n  df_sorted_id = df.sort('Date').withColumn('id', F.row_number().over(Window.orderBy('Date')))\n  \n  # Tricky part here: create a new column called 'cumsum' that will store the progressive number of consecutive NaN in our dataset.\n  # Let's break it into smaller parts:\n  # 1) create an index generator that will partition by 'Low' values [(...,Null...), (...,value1,...), (...,value2...), ... (...)] and within rows order by date\n  # Example:\n  ## |2019-06-05|null|null|  null|null| null|          null|624|        21|\n  ## |2019-06-06|null|null|  null|null| null|          null|625|        22|\n  ## |2019-06-07|null|null|  null|null| null|          null|626|        23| <- last column is the cumulative sum (i.e. the number of consecutive NaN)\n  ## ...\n  ## |2019-05-09|23.2|11.5|   7.3|4.2|  16.2|          29.1|1  |         0|\n  ## |2019-05-10|23.2|11.5|   7.3|4.2|  16.2|          29.1|2  |         0|\n  # 2) assign to each row a progressive index starting from 1 if it has null in correspondance of Low, zero otherwise\n  # 3) store these values into a new column called cumsum (i.e. it behaves like pandas cumsum)\n  # 4) at the end, the row whose ID corresponds to the length of the dataframe will contain at column 'cumsum' the no. of trailing NaN values.\n  cumsum_df = df_sorted_id.withColumn('cumsum', F.when(F.isnull(df_sorted_id.Low), F.row_number().over(Window.partitionBy('Low').orderBy('Date'))).otherwise(0))\n\n  # Retrieve the \"last\" row and read the value of cumsum\n  end_idx = cumsum_df.where(cumsum_df['id'] == df_length).first().cumsum\n  \n  # Retain rows whose index is lower len(df) - end_idx + 1 (i.e. cut trailing NaN values)\n  return df_sorted_id.where(df_sorted_id['id'] <= df_length-end_idx+1)\n\n\ndef merge_prices_fundamentals(\n    prices_dfs: List[pyspark.sql.DataFrame],\n    key_stats_df: pyspark.sql.DataFrame,\n    dfs_names: List[str],\n    drop_cols: List[str] = ['Date', 'Ticker', 'Price']\n    ) -> List[pyspark.sql.DataFrame]:\n  # Define the target list of dataframes\n  prices_dfs_new = []\n  for ticker in tqdm(key_stats_df.select('Ticker').distinct().collect(), desc='Merging the datasets ...'):\n    ticker = ticker[0]\n    # Consider only stocks for which we have fundamental data\n    if ticker.upper() not in dfs_names: continue\n      \n    # The prices dataframe associated to the current ticker\n    prices_df_idx = dfs_names.index(ticker.upper())\n    prices_df = prices_dfs[prices_df_idx]\n    \n    # Retrieve financial reports for the current ticker\n    ticker_df = key_stats_df.filter(F.col('Ticker') == ticker)\n    \n    # Perform an inner join between the two dataframes, we first take all reports\n    # whose date is at most the prices date (then we will take the latest available)\n    joined_df = ticker_df.join(prices_df, ticker_df.Date <= prices_df.Date, how='inner')\n    \n    # Let's break this into smaller parts:\n    # 1) Add a new column for the date latest available report for that stock. To do that\n    #    we partition over **prices** Date using the Window object, and the the max over\n    #    **ticker** Date.\n    # 2) Retain those columns whose financial report date is the same as in ticker_df_max_date\n    # 3) Finally drop the **ticker** Date and other unused columns\n    joined_df = joined_df.withColumn('ticker_df_max_date', F.max(ticker_df['Date'])\\\n                      .over(Window.partitionBy(prices_df['Date'])))\\\n                      .where(ticker_df['Date'] == F.col('ticker_df_max_date'))\\\n                      .drop('ticker_df_max_date', 'High', 'Low', 'Open', 'Close')\\\n                      .drop(ticker_df['Date'])\\\n                      \n    # Return the usual list of dataframes\n    prices_dfs_new.append(joined_df)\n    \n  return prices_dfs_new\n    \n                \ndef fill_missing_days(\n  aggregate_dfs: List[pyspark.sql.DataFrame],\n  remove_weekends: bool = True,\n  end_date: str = '2014-01-01'\n) -> List[pyspark.sql.DataFrame]:\n  result_dfs = []\n  \n  @udf(\"boolean\")\n  def is_weekday(date: datetime) -> bool:\n      ''' Returns true if the provided date corresponds to a weekday. '''\n      return date.weekday() < 5\n    \n  for df in tqdm(aggregate_dfs, desc='Filling missing days ...'):\n    # In this case we apply the following steps in order to apply fill-forward to our dataset:\n    # 1) define a Window object the will over the dataset sorted with decreasing dates\n    # 2) compute the difference in days between consecutive rows (11/05/2021 - 07/05/2021 ==> 4 days)\n    #    and store it in a column named diff\n    # 3) compute an increasing index in the column seq that is the days gap we need to fill\n    # Example:\n    # |Date      | Adjusted close   |diff|seq|new_date  |\n    # |2021-04-09|116.80999755859375|   3|  1|2021-04-11|\n    # |2021-04-09|116.80999755859375|   3|  2|2021-04-10|\n    # |2021-04-09|116.80999755859375|   3|  3|2021-04-09|\n    # 4) create a new column new_date as the result of the computation F.col('Date') + F.col('diff') - F.col('seq')\n    # 5) only retain rows whose date is before the provided end_date\n    #\n    # N.B. if we want to bfill the values we should instead consider the dataframes with dates in ascending order,\n    # and subtract the value of F.col('diff') instead of summing it. Other parts of the code stays the same.\n    df = df\\\n    .withColumn('diff', F.datediff(F.lag(F.col('Date'),1).over(Window.orderBy(F.desc('Date'))), F.col('Date')))\\\n    .withColumn('seq', F.explode(F.sequence(F.lit(1), F.col(\"diff\"))))\\\n    .withColumn('new_date', (F.col('Date') + F.col('diff') - F.col('seq')))\\\n    .where(F.col('new_date') < pd.Timestamp(end_date))\n    \n    # If specified, remove weekends that are most likely created by us as synthetic data with the ffill technique\n    if remove_weekends:\n      df = df.where(is_weekday(F.col('new_date')))\n      \n    # Drop unecessary columns\n    df = df.drop('Date', 'diff', 'seq').withColumnRenamed('new_date', 'Date')\n    \n    # If the dataframe is actually non-empty, add it to the list of resulting dataframes\n    if df.count() > 0:\n      result_dfs.append(df)\n  return result_dfs\n\ndef missing_values_summary(df):\n  ''' Returns a utility summary to view missing values in our dataframe. '''\n  n = df.count()\n  \n  def to_percentage(x: pyspark.sql.column.Column, n: int) -> int:\n    ''' Utility function to compute the amount of missing values as a percentage of the original dataframe. '''\n    return F.round(100 * x / n, 3)\n  \n  # Aggregate using the count function over null values, and return a view over the obtained (single row) dataframe\n  return df.agg(*[to_percentage(F.count(F.when(F.isnull(c), c)), n).alias(c) for c in df.columns]).first()\n\ndef scale_features(dfs: List[pyspark.sql.DataFrame]):\n    ''' Scales the numerical features to unit variance. '''\n    \n    scalable_features = ['DE Ratio', 'Trailing P/E', 'Price/Sales', 'Price/Book',\n       'Profit Margin', 'Operating Margin', 'Return on Assets',\n       'Return on Equity', 'Revenue Per Share', 'Market Cap',\n       'Enterprise Value', 'PEG Ratio', 'Enterprise Value/Revenue',\n       'Enterprise Value/EBITDA', 'Revenue', 'Gross Profit', 'EBITDA',\n       'Net Income Avl to Common ', 'Diluted EPS', 'Earnings Growth',\n       'Revenue Growth', 'Total Cash', 'Total Cash Per Share', 'Total Debt',\n       'Current Ratio', 'Book Value Per Share', 'Cash Flow', 'Beta', 'Volume',\n       'Adjusted Close', 'SMA', 'RSI']\n    \n\n    # Aggregating dfs in a single one\n    aggregate_df = dfs[0]\n    for df in tqdm(dfs[1:]):\n      aggregate_df = aggregate_df.union(df)\n      \n    # Computing means and stds for every column of the aggregate df\n    mean = aggregate_df.select(\n      [F.mean(F.col(column)) for column in scalable_features]\n    ).collect()[0]\n    \n    std = aggregate_df.select(\n      [F.stddev(F.col(column)) for column in scalable_features]\n    ).collect()[0]\n    \n    #summary = aggregate_df.select(scalable_features).summary(\"mean\", \"stddev\")\n    #mean = summary.filter(summary[\"summary\"] == \"mean\")#.select(scalable_features)\n    #std = summary.filter(summary[\"summary\"] == \"std\")\n    #print(mean)\n\n    \n    for i in tqdm(range(len(dfs)), desc='Scaling numerical features ...'):\n      # Scaling scalable features\n      for j, feature in enumerate(scalable_features):\n        dfs[i] = dfs[i].withColumn(feature, (dfs[i][feature]-mean[j])/std[j])\n        #dfs[i] = dfs[i].withColumn(feature, (dfs[i][feature]-mean[feature])/std[feature])\n    return mean[-3], std[-3]\n        \ndef impute_missing_values(\n  prices_dfs_new: List[pyspark.sql.DataFrame],\n  key_stats_df: pyspark.sql.DataFrame\n) -> Tuple[List[pyspark.sql.DataFrame], pyspark.sql.DataFrame]:\n  # define the window\n  window = Window.orderBy('Date').rowsBetween(Window.unboundedPreceding, 0)\n\n  # Forward filling values \n  # (ref. https://stackoverflow.com/questions/38131982/forward-fill-missing-values-in-spark-python/50422240#50422240)\n  for i in range(len(prices_dfs_new)):\n    for col_name in prices_dfs_new[i].schema.names:\n      col = F.last(prices_dfs_new[i][col_name], ignorenulls=True).over(window)\n      prices_dfs_new[i] = prices_dfs_new[i].withColumn(col_name, col)\n\n  # In this case this dataframe contains financial reports that may contain NaN values either because that\n  # metric was not available at that time OR because it was monitoring an initial stage of a company growth.\n  # What we do is to apply the classic fast-forward, and fill initial missing values with zeroes.\n  # Please note: we also discard the 'Forward P/E' column since the imputation here would introduce too much noise.\n  key_stats_df_new = key_stats_df.drop('Forward P/E')\n  for col_name in key_stats_df_new.schema.names:\n      col = F.last(key_stats_df_new[col_name], ignorenulls=True).over(window)\n      key_stats_df_new = key_stats_df_new.withColumn(col_name, col)\n  key_stats_df_new = key_stats_df_new.fillna(0.)\n  \n  return prices_dfs_new, key_stats_df_new\n\ndef from_dfs(\n  dfs: List[pyspark.sql.DataFrame],\n  window_size: int = 3,\n  target_col: str = 'Adjusted Close',\n  val_date: str = '2012-01-01',\n  test_date: str = '2013-01-01'\n):\n  # Split the dataset into train/val/test using these dates as boundaries\n  val_begin = pd.Timestamp(val_date)\n  test_begin = pd.Timestamp(test_date)\n  \n  def should_drop(col: pyspark.sql.column, idx: int) -> bool:\n    ''' Returns whether the provided column should be dropped. '''\n    return (col.endswith(f'_{idx}') or 'Ticker' in col or 'Date' in col or 'id' in col)\\\n              and col != f'Adjusted Close_{idx}'\n\n  train_split, val_split, test_split = [], [], []\n  \n  for df in tqdm(dfs, desc='Loading dataset from dfs ...'):\n    # Create an ID for each row based on dates\n    df = df\\\n      .sort('Date')\\\n      .withColumn('id', F.row_number().over(Window.orderBy('Date')))\n    \n    # Split the stock dataset into train/dev/test splits\n    train_df = df.where(F.col('Date') < val_begin)\n    val_df = df.where(F.col('Date').between(val_begin, test_begin-pd.Timedelta(days=1)))\n    test_df = df.where(F.col('Date') >= test_begin)\n    \n    # Add a unique identifier for each column (e.g. Date -> Date_0, Date_1, etc ...)\n    train_df = prepend_columns_index(train_df)\n    val_df = prepend_columns_index(val_df)\n    test_df = prepend_columns_index(test_df)\n    \n    # Convert each split from nxm -> nxm*(w+1), where w is the size of the window\n    train_df = create_window_indexes(train_df, window_size)\n    val_df = create_window_indexes(val_df, window_size)\n    test_df = create_window_indexes(test_df, window_size)\n    \n    # Collect the processed datasets\n    train_split.append(train_df)\n    val_split.append(val_df)\n    test_split.append(test_df)\n  \n  # Initialize the final datasets using the obtained df schema\n  train_ds = spark.createDataFrame([], train_split[0].schema)\n  val_ds = spark.createDataFrame([], val_split[0].schema)\n  test_ds = spark.createDataFrame([], test_split[0].schema)\n  \n  # Collect all the dataframes into a single df for each data split\n  for df in train_split: train_ds = train_ds.union(df)\n  for df in val_split: val_ds = val_ds.union(df)\n  for df in test_split: test_ds = test_ds.union(df)\n  \n  # Drop unused columns for the target variable and rename the target column as 'y'\n  drop_cols = [col for col in test_ds.columns if should_drop(col, window_size)]\n  train_ds = train_ds.drop(*drop_cols).withColumnRenamed(f'Adjusted Close_{window_size}', 'y')\n  val_ds = val_ds.drop(*drop_cols).withColumnRenamed(f'Adjusted Close_{window_size}', 'y')\n  test_ds = test_ds.drop(*drop_cols).withColumnRenamed(f'Adjusted Close_{window_size}', 'y')\n  \n  # Rename columns to make them compatible with Apache Parquet format\n  target_cols = list(map(lambda x: '_'.join(re.split(' |/', x)), train_ds.columns))\n  for c, target in zip(train_ds.columns, target_cols): train_ds = train_ds.withColumnRenamed(c, target)\n  for c, target in zip(val_ds.columns, target_cols): val_ds = val_ds.withColumnRenamed(c, target)\n  for c, target in zip(test_ds.columns, target_cols): test_ds = test_ds.withColumnRenamed(c, target)\n    \n  # Compute the set of unique features in each split\n  features = list(set(c[:-2] for c in train_ds.columns if len(c) >= 2))\n\n  # Convert each split to have, at each timestep (column), the collection of features at that timestep\n  for timestep in range(window_size):\n    # Consider all the features in this timestep\n    current_cols = list(map(lambda c: f'{c}_{timestep}', features))\n    # Convert (price_0, sma_0, market_cap_0) -> 0: (price|sma|market_cap) \n    train_ds = train_ds.withColumn(f'x_{timestep}', F.array(current_cols)).drop(*current_cols)\n    val_ds = val_ds.withColumn(f'x_{timestep}', F.array(current_cols)).drop(*current_cols)\n    test_ds = test_ds.withColumn(f'x_{timestep}', F.array(current_cols)).drop(*current_cols)\n  \n  return train_ds, val_ds, test_ds, features\n\ndef prepend_columns_index(\n  df: pyspark.sql.DataFrame,\n  idx: int = 0\n) -> pyspark.sql.DataFrame:\n  ''' Utility function that returns a new dataframe where columns are preceeded by [idx]. '''\n  for col in df.schema.names:\n    # If this is not the first time this function is applied, simply edit the previous identifier\n    new_col = f'{col}_{idx}' if idx == 0 else f'{col[:-2]}_{idx}'\n    df = df.withColumnRenamed(col, new_col)\n  return df\n\ndef create_window_indexes(df: pyspark.sql.DataFrame, window_size: int) -> pyspark.sql.DataFrame:\n  ''' Apply self join multiple times to the input dataframe, return a new one of size (df.count(), len(df.columns)*window_size) '''\n  joined_df = df\n  for win_idx in range(1, window_size+1):\n    joined_df = joined_df.join(\n      prepend_columns_index(df, win_idx),\n      F.col('id_0') == F.col(f'id_{win_idx}') - win_idx\n    )\n  return joined_df\n\ndef collate_fn(\n  time_steps: List[str],\n  batch: List[Dict[str, np.array]]\n) -> Tuple[torch.Tensor, torch.Tensor]:\n  ''' Returns (x, y) pairs where x is a vector of shape: (window_size, feature_size). '''\n  X = np.asarray([np.stack(itemgetter(*time_steps)(d)) for d in batch])\n  Y = np.asarray([d['y'] for d in batch])\n  return torch.from_numpy(X).double(), torch.from_numpy(Y).double()\n\ndef data_loader_fn(window_size: int, **kwargs) -> DataLoader:\n  ''' Override default behaviour and return a petastorm.Dataset with custom collate function. '''\n  time_steps = [f'x_{time_step}' for time_step in range(window_size)]\n  return DataLoader(**kwargs, collate_fn=partial(collate_fn, time_steps))\n\ndef r2_score(y_hat: np.ndarray, y: np.ndarray) -> float:\n  ''' Computes the R Squared coefficient between y_hat and y. '''\n  rss = torch.sum((y - y_hat) ** 2)\n  tss = torch.sum((y - y.mean()) ** 2)\n  return 1 - rss / tss\n\ndef adjusted_r2_score(y_hat: np.ndarray, y: np.ndarray, p: int) -> float:\n  ''' Computes the Adjusted R Squared coefficient between y_hat and y. '''\n  rss = torch.sum((y - y_hat) ** 2)\n  tss = torch.sum((y - y.mean()) ** 2)\n  df_e = y.shape[0] - p - 1\n  df_t = y.shape[0] - 1\n  return 1 - (rss / tss) * (df_t / df_e)\n  \n\ndef plot_loss_history(loss_history):\n  ''' Plots the loss history. '''\n  plt.plot(loss_history['train_history'], label='Train loss')\n  plt.plot(loss_history['valid_history'], label='Valid loss')\n  plt.xlabel('Epochs')\n  plt.ylabel('Loss')\n  plt.legend(loc=\"upper right\")\n  plt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebf7570c-7796-4442-9a41-c7a8e9069d60"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Technical indicators"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"707b227e-4c9c-4943-b14a-db5253fec795"}}},{"cell_type":"code","source":["def add_sma(dfs: List[pyspark.sql.DataFrame], period: int = 10) -> None:\n    ''' Computes the Simple Moving Average from a given dataframe. '''\n    # Window of previous [period] days.\n    w = (Window().orderBy(F.col(\"timestamp\").cast('long')).rangeBetween(-period*86400, 0))\n    for i in tqdm(range(len(dfs)), desc='Adding SMA ...'):\n        # Adding temporary timestamp column\n        dfs[i] = dfs[i].withColumn('timestamp', dfs[i].Date.cast('timestamp'))\n        # Computing moving average over the window\n        dfs[i] = dfs[i].withColumn(\n          \"SMA\",\n          F.avg(\"Adjusted Close\").over(w)\n        )\n        # Dropping temporary columns\n        dfs[i] = dfs[i].drop(\"timestamp\")\n\n        \ndef add_rsi(dfs: List[pyspark.sql.DataFrame], period: int = 14) -> None:\n    ''' Computes the Relative Strength Index from a given dataframe. \n        Formula available at https://en.wikipedia.org/wiki/Relative_strength_index.\n        Also adds overbought and oversold when the RSI index hits 70 or 30.'''\n    for j in tqdm(range(len(dfs)), desc='Adding RSI ...'):\n        \n        #dateCol = dfs[j][\"Date\"]\n        w = Window.orderBy(\"Date\")\n\n        # Lagging the adjusted close of one row (to have the price of the previous day)\n        dfs[j] = dfs[j].withColumn(\"prev_adj_close\", F.lag(dfs[j][\"Adjusted Close\"]).over(w))\n        # Computing day gain and day loss\n        dfs[j] = dfs[j].withColumn(\"current_gain\", F.when(dfs[j][\"Adjusted Close\"] >= dfs[j][\"prev_adj_close\"],\n                                                          dfs[j][\"Adjusted Close\"] - dfs[j][\"prev_adj_close\"])\n                                                          .otherwise(0.0))\n        dfs[j] = dfs[j].withColumn(\"current_loss\", F.when(dfs[j][\"prev_adj_close\"] >= dfs[j][\"Adjusted Close\"],\n                                                         dfs[j][\"prev_adj_close\"] - dfs[j][\"Adjusted Close\"])\n                                                         .otherwise(0.0))\n        \n        # Adding temporary timestamp column\n        dfs[j] = dfs[j].withColumn('timestamp', dfs[j].Date.cast('timestamp'))\n        # Window of previous [period] days.\n        w = (Window().orderBy(F.col(\"timestamp\").cast('long')).rangeBetween(-period*86400, 0))\n        # Computing moving averages of day gain and day loss over the window\n        dfs[j] = dfs[j].withColumn(\n          \"smmau\",\n          F.avg(\"current_gain\").over(w)\n        )\n        \n        dfs[j] = dfs[j].withColumn(\n          \"smmad\",\n          F.avg(\"current_loss\").over(w)\n        )\n        \n        # Computing RSI given the moving averages\n        dfs[j] = dfs[j].withColumn(\"RSI\", F.when(dfs[j][\"smmau\"] == 0.0, 50.0).when(dfs[j][\"smmad\"] == 0.0, 50.0).otherwise(100 - (100/(1+(dfs[j][\"smmau\"]/dfs[j][\"smmad\"])))))            \n        # Adding overbought and oversold signals when the RSI goes above 70 and below 30\n        dfs[j] = dfs[j].withColumn(\"Overbought\", F.when(dfs[j][\"RSI\"] >= 70, 1.0).otherwise(0.0))\n        dfs[j] = dfs[j].withColumn(\"Oversold\", F.when(dfs[j][\"RSI\"] <= 30, 1.0).otherwise(0.0))\n        #dfs[j] = dfs[j].withColumn(\"Date\", dateCol)\n        \n        # Dropping temporary columns\n        dfs[j] = dfs[j].drop(\"prev_adj_close\", \"current_gain\", \"current_loss\", \"smmau\", \"smmad\", \"timestamp\")\n        \ndef fit_evaluate(\n  model_name: str,\n  window_size: int,\n  train_df: pyspark.sql.DataFrame,\n  test_df: pyspark.sql.DataFrame\n) -> None:\n  ''' Fits a model on train data and evaluate on test data. '''\n  features = [f'x_{time_step}' for time_step in range(window_size)]\n  train_df = train_df.withColumn('x', F.concat(*features)).drop(*features)\n  test_df = test_df.withColumn('x', F.concat(*features)).drop(*features)\n  \n  list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n  train_df_x = list_to_vector_udf(train_df['x']).alias('x')\n  test_df_x = list_to_vector_udf(test_df['x']).alias('x')\n  \n  train_df = train_df.select(train_df_x, train_df['y'])\n  test_df = test_df.select(test_df_x, test_df['y'])\n  if model_name == 'LinearRegression':\n    lr = LinearRegression(featuresCol='x', labelCol='y')  \n    model = lr.fit(train_df)  \n  \n  #model.transform(test_df).show(5)\n  test_result = model.evaluate(test_df)\n  \n  print(f'Performance with {model_name}')\n  print('RMSE: {:.3f}'.format(test_result.rootMeanSquaredError))\n  print('R2: {:.3f}'.format(test_result.r2))\n  print('Adjusted R2: {:.3f}'.format(test_result.r2adj))\n  \n@torch.no_grad()\ndef evaluate_trading_strategy(\n  model: nn.Module,\n  test_dataloader: DataLoader,\n  mean: float,\n  std: float,\n  stock_num: int\n) -> None:\n  pnl = 0.0\n  tries = 0\n  correct = 0\n  for X, y in test_dataloader:\n    y_pred = model(X.float())\n    # Re-adding mean to prices to make them positive\n    X[:, :, -5] = X[:, :, -5] * std + mean\n    y_pred = y_pred * std + mean\n    y = y * std + mean\n    tries += X.shape[0]\n    for i in range(X.shape[0]):\n      # Increase predicted\n      if X[i, -1, -5] < y_pred[i]:\n        # Adding long profit / loss\n        ret = y[i]/X[i, -1, -5] - 1.0\n      # Decrease predicted\n      else:\n        # Adding short profit / loss\n        ret = X[i, -1, -5]/y[i] - 1.0\n      pnl += ret\n      # Counting correct operations \n      correct += 1 if ret > 0 else 0\n\n  print(f\"Average 1-year profit: {pnl.item()/stock_num*100}%\")\n  print(f\"Operation accuracy: {(correct/tries)*100}%\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de4a5d87-ba54-4970-8ecb-54e9ad305164"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"Overview of the missing values in the key_stats dataframe\\n\")\nkey_stats_summary = missing_values_summary(key_stats_df)\nkey_stats_summary"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a201af9-db34-4924-81d9-4982833e72e1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Missing values imputation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db3487d2-05a9-43f2-a9d0-1feb1ece31f9"}}},{"cell_type":"code","source":["summary = prices_df_nan_summary(prices_dfs, dfs_names)\npx.bar(summary, x='Stock name', y='Missing data (%)', hover_data=['Count'], title=\"Stock price dataset before preprocessing (only columns with missing values are displayed)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c77cdd70-d8af-43db-b097-f9ce91f1c266"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["For most of the above stocks with missing values, we noticed that they indeed exist up to a given time and after that no more data is available. It may due to a business failure, hence no more stocks will be exchanged from that moment on."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46156c58-76a8-40c8-88a3-b4bf0a800dd5"}}},{"cell_type":"code","source":["# Clear our input data from training NaN values\nprices_dfs_new = [remove_trailing_nan(df,name) for df,name in tqdm(zip(prices_dfs, dfs_names), total=len(prices_dfs), desc='Removing trailing NaN values ...')]\n\n# Remove INTH stock from our dataset since it contains many inactivity periods\n#inth_idx = dfs_names.index('INTH') #TODO: uncomment\n#del dfs_names[inth_idx] #TODO: uncomment\n#del prices_dfs_new[inth_idx] #TODO: uncomment\n\nsummary = prices_df_nan_summary(prices_dfs_new, dfs_names)\npx.bar(summary, x='Stock name', y='Missing data (%)', hover_data=['Count'], title=\"Stock price dataset after preprocessing (only columns with missing values are displayed)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5884e89a-4386-4042-bd50-39dd70e3cef8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["At this point we use the fast forward imputation technique to fill-in missing values. Please note that in this case missing values are mostly due to holidays or periods when stocks are not exchanged."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b219a552-fd13-453f-826c-71fb94bfd81f"}}},{"cell_type":"markdown","source":["## Hyperparameters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c47e3765-14b1-4228-a3b8-51985f53cd14"}}},{"cell_type":"code","source":["@dataclasses.dataclass\nclass HParams:\n  input_dim: int = -1  # Denoted by data transformations\n  window_size: int = 5\n  batch_size: int = 1024\n  features: int = -1  # Denoted by data transformations\n  hidden_dim: int = 128\n  num_layers: int = 1\n  dropout: int = 0.5\n  lr: int = 0.1\n    \nhparams = HParams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0fb8282-de11-4c4a-86ba-39b556d9b988"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Building our new dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3595c39e-c759-41d4-a927-ab75e577b16e"}}},{"cell_type":"code","source":["# Impute missing values in the prices dataset (i.e. fast-forward last valid values)\nprices_dfs_new, key_stats_df_new = impute_missing_values(prices_dfs_new, key_stats_df)\n\n# Merge the stock price dataset with fundamental data of the relative company\naggregate_dfs = merge_prices_fundamentals(prices_dfs_new, key_stats_df_new, dfs_names)\n\n# Fill gaps from the original dataset\n#dfs = fill_missing_days(aggregate_dfs)\ndfs = aggregate_dfs\n                  \n# Add SMA indicator to each dataframe\nadd_sma(dfs)\n\n# Add RSI indicator to each dataframe\nadd_rsi(dfs)\n\n# Scale numerical features\nprice_mean, price_std = scale_features(dfs)\n\n#print(dfs[1].select(\"Date\",\"RSI\", \"Adjusted Close\", \"Overbought\", \"Oversold\").show(100000))\n#dfs[0].show(500)\n\n# Create train/dev/test splits\ntrain_ds, val_ds, test_ds, features = from_dfs(dfs, window_size=hparams.window_size)\nhparams.input_dim = len(features)\nhparams.features = features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b215510b-4ef8-4b04-b097-c467153c4c84"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##Models definition"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b15deec7-b5a2-49db-a08d-94bd3145ed1c"}}},{"cell_type":"code","source":["def r2_score(y_hat: np.ndarray, y: np.ndarray) -> float:\n    ''' Computes the R Squared coefficient between y_hat and y. '''\n    rss = torch.sum((y - y_hat) ** 2)\n    tss = torch.sum((y - y.mean()) ** 2)\n    return 1 - rss / tss"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c471a6b1-4862-47c0-988a-ea3642c611e0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class SimpleLSTM(nn.Module):\n  def __init__(self, hparams):\n      super().__init__()\n      self.hparams = hparams\n      self.lstm = nn.LSTM(hparams.input_dim, hparams.hidden_dim, num_layers=hparams.num_layers, batch_first=True)\n      self.fc1 = nn.Linear(hparams.hidden_dim, hparams.hidden_dim//2)\n      self.fc2 = nn.Linear(hparams.hidden_dim//2, 1)\n      self.dropout = nn.Dropout(hparams.dropout)\n\n  def forward(self, x: np.ndarray):\n      x, (h,c) = self.lstm(x)\n      h = self.dropout(h)\n      h = h.view(self.hparams.num_layers, -1, self.hparams.hidden_dim)[-1]\n      x = torch.relu(self.fc1(h))\n      x = self.fc2(x)\n      return x.squeeze()\n\ndef train(\n  model: nn.Module,\n  optimizer: torch.optim.SGD,\n  train_dataloader: DataLoader,\n  valid_dataloader: DataLoader,\n  logger,\n  epochs: int = 5\n) -> None:\n  train_history = []\n  valid_history = []\n  mean_r2 = 0.0\n\n  progress = tqdm(range(epochs))\n  progress.set_description(f\"Epoch: {0}, train loss: 0, val loss: 0\")\n  for epoch in progress:\n      losses = []\n      r2_ls = []\n\n      for x, y in train_dataloader:\n          # Zero the gradients to prevent Pytorch from accumulating the gradients\n          optimizer.zero_grad()\n          y_hat = model(x.float())\n\n          # Compute the loss\n          loss = loss_fn(y_hat, y.float())\n          losses.append(loss)\n\n\n          # Compute the gradient of the loss and update model parameters\n          loss.backward()\n          optimizer.step()\n\n      mean_loss = sum(losses) / len(losses)\n      train_history.append(mean_loss.item())\n\n      losses = []\n      with torch.no_grad():\n          for x, y in valid_dataloader:\n              y_hat = model(x.float())\n              loss = loss_fn(y_hat, y.float())\n              losses.append(loss)\n              r2_ls.append(r2_score(y_hat, y))\n\n\n\n      mean_loss = sum(losses) / len(losses)\n      valid_history.append(mean_loss.item())\n      mean_r2 = sum(r2_ls) / len(r2_ls)\n      \n      logger.log(\n        {\n          'train_loss': train_history[-1],\n          'valid_loss': valid_history[-1],\n          'r2': mean_r2,\n        }\n      )\n      \n      progress.set_description(f\"Epoch: {epoch+1}, train loss: {train_history[-1]}, val loss: {valid_history[-1]} r2 {mean_r2}\")\n\n  return {\n      'train_history': train_history,\n      'valid_history': valid_history\n  }\n\n@torch.no_grad()\ndef evaluate(model, dataloader):\n    Y = []\n    Y_pred = []\n\n    for x, y in dataloader:\n        y_hat = model(x.float())\n        Y.append(y)\n        Y_pred.append(y_hat)\n\n    return {\n        'y_true': torch.stack(Y).reshape(-1),\n        'y_pred': torch.stack(Y_pred).reshape(-1).round()\n    }"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92ce704a-ec3c-43f9-b503-6fc7bf2aaa65"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Define the number of epochs to run the model on\nhparams.epochs = 5\nhparams.lr = 0.1\n\n# Configure the current run\nrun = wandb.init(reinit=True, project='distributed_spf')\n\n# Define the loss function and the model's optimizer\nloss_fn = nn.MSELoss()\nmodel = SimpleLSTM(hparams)\noptimizer = torch.optim.SGD(model.parameters(), lr=hparams.lr)\n\n# Materialize spark dataframes to DBFS in parqet format\nconverter_train = make_spark_converter(train_ds)\nconverter_valid = make_spark_converter(val_ds)\nconverter_test = make_spark_converter(test_ds)\n\n# Fit the model on training data, and evaluate it on validation split\nwith converter_train.make_torch_dataloader(\n  batch_size=hparams.batch_size, data_loader_fn=partial(data_loader_fn, hparams.window_size), num_epochs=1\n) as train_dataloader, converter_valid.make_torch_dataloader(\n  batch_size=hparams.batch_size, data_loader_fn=partial(data_loader_fn, hparams.window_size), num_epochs=1\n) as valid_dataloader, converter_test.make_torch_dataloader(\n  batch_size=hparams.batch_size, data_loader_fn=partial(data_loader_fn, hparams.window_size), num_epochs=1\n) as test_dataloader:\n  \n  train_logs = train(model, optimizer, train_dataloader, valid_dataloader, run, epochs=hparams.epochs)\n  plot_loss_history(train_logs)\n  \n  evaluate_trading_strategy(model, test_dataloader, price_mean, price_std, len(dfs))\n\n# Log the experiments\nrun.finish()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08c49aca-c864-49b2-9a5a-787531af03bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;IPython.core.display.HTML object&gt;\nThe median size 844621 B (&lt; 50 MB) of the parquet files is too small. Total size: 4032659 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:/dbfs/tmp/petastorm/cache/20210518161954-appid-local-1621354066970-6ff4d400-5655-4159-929e-47e711af77f5/part-00000-tid-502487384480357514-6acbdd0e-22d6-4b1b-8268-509d7730136c-422-1-c000.parquet, ...\nThe median size 87604 B (&lt; 50 MB) of the parquet files is too small. Total size: 428963 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:/dbfs/tmp/petastorm/cache/20210518162231-appid-local-1621354066970-4d7f1e01-089e-4516-bb57-8d9cca40d875/part-00010-tid-3120548432593701661-4d80670b-a628-425b-b055-1a6dd07bc88f-592-1-c000.parquet, ...\nThe median size 855165 B (&lt; 50 MB) of the parquet files is too small. Total size: 5046051 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:/dbfs/tmp/petastorm/cache/20210518162500-appid-local-1621354066970-6cfb1678-d319-40d1-80aa-5e6f0148cea8/part-00009-tid-6379229041322519299-7dfaebae-8b40-4332-a1ab-e085dccb9a18-751-1-c000.parquet, ...\n\r  0%|          | 0/5 [00:00&lt;?, ?it/s]\rEpoch: 0, train loss: 0, val loss: 0:   0%|          | 0/5 [00:00&lt;?, ?it/s]\rEpoch: 1, train loss: 0.15106341242790222, val loss: 0.11053349077701569 r2 -23.22614891378901:   0%|          | 0/5 [00:02&lt;?, ?it/s]\rEpoch: 1, train loss: 0.15106341242790222, val loss: 0.11053349077701569 r2 -23.22614891378901:  20%|        | 1/5 [00:02&lt;00:08,  2.23s/it]Start a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\nStart a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\n\rEpoch: 2, train loss: 0.06246136501431465, val loss: 0.18664738535881042 r2 -2.187969097954324:  20%|        | 1/5 [00:04&lt;00:08,  2.23s/it]\rEpoch: 2, train loss: 0.06246136501431465, val loss: 0.18664738535881042 r2 -2.187969097954324:  40%|      | 2/5 [00:04&lt;00:06,  2.25s/it]Start a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\nStart a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\n\rEpoch: 3, train loss: 0.043682996183633804, val loss: 0.13643968105316162 r2 -0.36355004699375626:  40%|      | 2/5 [00:06&lt;00:06,  2.25s/it]\rEpoch: 3, train loss: 0.043682996183633804, val loss: 0.13643968105316162 r2 -0.36355004699375626:  60%|    | 3/5 [00:06&lt;00:04,  2.23s/it]Start a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\nStart a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\n\rEpoch: 4, train loss: 0.043410953134298325, val loss: 0.3164252042770386 r2 -2.0376020036472875:  60%|    | 3/5 [00:08&lt;00:04,  2.23s/it]  \rEpoch: 4, train loss: 0.043410953134298325, val loss: 0.3164252042770386 r2 -2.0376020036472875:  80%|  | 4/5 [00:08&lt;00:02,  2.19s/it]Start a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\nStart a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\n\rEpoch: 5, train loss: 0.03793233260512352, val loss: 0.11297660320997238 r2 -0.2192070143275192:  80%|  | 4/5 [00:11&lt;00:02,  2.19s/it]\rEpoch: 5, train loss: 0.03793233260512352, val loss: 0.11297660320997238 r2 -0.2192070143275192: 100%|| 5/5 [00:11&lt;00:00,  2.25s/it]\rEpoch: 5, train loss: 0.03793233260512352, val loss: 0.11297660320997238 r2 -0.2192070143275192: 100%|| 5/5 [00:11&lt;00:00,  2.24s/it]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;IPython.core.display.HTML object&gt;\nThe median size 844621 B (&lt; 50 MB) of the parquet files is too small. Total size: 4032659 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:/dbfs/tmp/petastorm/cache/20210518161954-appid-local-1621354066970-6ff4d400-5655-4159-929e-47e711af77f5/part-00000-tid-502487384480357514-6acbdd0e-22d6-4b1b-8268-509d7730136c-422-1-c000.parquet, ...\nThe median size 87604 B (&lt; 50 MB) of the parquet files is too small. Total size: 428963 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:/dbfs/tmp/petastorm/cache/20210518162231-appid-local-1621354066970-4d7f1e01-089e-4516-bb57-8d9cca40d875/part-00010-tid-3120548432593701661-4d80670b-a628-425b-b055-1a6dd07bc88f-592-1-c000.parquet, ...\nThe median size 855165 B (&lt; 50 MB) of the parquet files is too small. Total size: 5046051 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:/dbfs/tmp/petastorm/cache/20210518162500-appid-local-1621354066970-6cfb1678-d319-40d1-80aa-5e6f0148cea8/part-00009-tid-6379229041322519299-7dfaebae-8b40-4332-a1ab-e085dccb9a18-751-1-c000.parquet, ...\n\r  0%|          | 0/5 [00:00&lt;?, ?it/s]\rEpoch: 0, train loss: 0, val loss: 0:   0%|          | 0/5 [00:00&lt;?, ?it/s]\rEpoch: 1, train loss: 0.15106341242790222, val loss: 0.11053349077701569 r2 -23.22614891378901:   0%|          | 0/5 [00:02&lt;?, ?it/s]\rEpoch: 1, train loss: 0.15106341242790222, val loss: 0.11053349077701569 r2 -23.22614891378901:  20%|        | 1/5 [00:02&lt;00:08,  2.23s/it]Start a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\nStart a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\n\rEpoch: 2, train loss: 0.06246136501431465, val loss: 0.18664738535881042 r2 -2.187969097954324:  20%|        | 1/5 [00:04&lt;00:08,  2.23s/it]\rEpoch: 2, train loss: 0.06246136501431465, val loss: 0.18664738535881042 r2 -2.187969097954324:  40%|      | 2/5 [00:04&lt;00:06,  2.25s/it]Start a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\nStart a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\n\rEpoch: 3, train loss: 0.043682996183633804, val loss: 0.13643968105316162 r2 -0.36355004699375626:  40%|      | 2/5 [00:06&lt;00:06,  2.25s/it]\rEpoch: 3, train loss: 0.043682996183633804, val loss: 0.13643968105316162 r2 -0.36355004699375626:  60%|    | 3/5 [00:06&lt;00:04,  2.23s/it]Start a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\nStart a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\n\rEpoch: 4, train loss: 0.043410953134298325, val loss: 0.3164252042770386 r2 -2.0376020036472875:  60%|    | 3/5 [00:08&lt;00:04,  2.23s/it]  \rEpoch: 4, train loss: 0.043410953134298325, val loss: 0.3164252042770386 r2 -2.0376020036472875:  80%|  | 4/5 [00:08&lt;00:02,  2.19s/it]Start a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\nStart a new pass of Petastorm DataLoader, reset underlying Petastorm reader to position 0.\n\rEpoch: 5, train loss: 0.03793233260512352, val loss: 0.11297660320997238 r2 -0.2192070143275192:  80%|  | 4/5 [00:11&lt;00:02,  2.19s/it]\rEpoch: 5, train loss: 0.03793233260512352, val loss: 0.11297660320997238 r2 -0.2192070143275192: 100%|| 5/5 [00:11&lt;00:00,  2.25s/it]\rEpoch: 5, train loss: 0.03793233260512352, val loss: 0.11297660320997238 r2 -0.2192070143275192: 100%|| 5/5 [00:11&lt;00:00,  2.24s/it]\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/plots/61e44874-d8e4-42f9-8c57-31d30c8e26c3.png","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"image","arguments":{}}},"output_type":"display_data","data":{"image/png":"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"}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Average 1-year profit: 375864.37775354297%\nOperation accuracy: 70.23270846800258%\nRun pip install nbformat to save notebook history\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Average 1-year profit: 375864.37775354297%\nOperation accuracy: 70.23270846800258%\nRun pip install nbformat to save notebook history\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n&lt;IPython.core.display.HTML object&gt;\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["run"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8dcd88b6-8d42-4101-9e54-1398f427ee9d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[131]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[131]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<h1>Run(p72r185j)</h1><iframe src=\"https://wandb.ai/leonardoemili/distributed_spf/runs/p72r185j\" style=\"border:none;width:100%;height:400px\"></iframe>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<h1>Run(p72r185j)</h1><iframe src=\"https://wandb.ai/leonardoemili/distributed_spf/runs/p72r185j\" style=\"border:none;width:100%;height:400px\"></iframe>"]}}],"execution_count":0},{"cell_type":"code","source":["fit_evaluate('LinearRegression', hparams.window_size, train_ds, test_ds)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0388bb22-41e8-4f62-a3cd-a6bf6f6f2e5f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"dist_forecasting","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1763883994012516}},"nbformat":4,"nbformat_minor":0}
